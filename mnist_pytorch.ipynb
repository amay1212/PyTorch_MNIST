{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "rkTg4cXT9pjW",
        "colab_type": "code",
        "outputId": "c7261a83-280e-4062-91e4-b22160a0a51b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        " \n",
        "  \n",
        "  #Converting the input to a PNG image.\n",
        "'''  \n",
        "Note : \n",
        "Converted to PNG\n",
        "Trimmed\n",
        "Resized\n",
        "Sharpened\n",
        "Extended\n",
        "Negated\n",
        "Gray-scaled'''\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  \\nNote : \\nConverted to PNG\\nTrimmed\\nResized\\nSharpened\\nExtended\\nNegated\\nGray-scaled'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "3XPQcyP3-7jj",
        "colab_type": "code",
        "outputId": "fc2b2e19-399c-45f1-debb-335ed2b8fd59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Importing libraries first to play with the dataset..\n",
        "#Getting started with building our first neural nw...\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision as vision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from plotcm import plot_confusion_matrix\n",
        "\n",
        "import pdb\n",
        "\n",
        "torch.set_printoptions(linewidth=120)\n",
        "print('Import done!!')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Import done!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WF6jBvdc_Uc9",
        "colab_type": "code",
        "outputId": "00a37471-a53d-473f-fabb-fcd70488f6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "We will be using the fashion-MNIST dataset that comes built-in with the torchvision package,\n",
        "so we wonâ€™t have to do this for our project. Just know that the Fashion-MNIST built-in dataset class is doing this behind the scenes.\n",
        "'''\n",
        "\n",
        "\n",
        "#Torchvivion has the toy datatset for MNIST and FashionMNIST\n",
        "'''\n",
        "An abstract class is a Python class that has methods we must implement, so we can \n",
        "create a custom dataset by creating a subclass that extends the functionality of the\n",
        "Dataset class.\n",
        "To create a custom dataset using PyTorch, we extend the Dataset class by creating\n",
        "a subclass that \n",
        "implements these required methods. Upon doing this, our new subclass can then \n",
        "be passed to the a PyTorch DataLoader object.\n",
        "\n",
        "Fashion-MNIST dataset extends the Datset class internally . therefore available in torchvision package..\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#Creating a Train set\n",
        "\n",
        "train_set = vision.datasets.FashionMNIST(\n",
        "              root='./data/FashionMNIST',\n",
        "              train = True , \n",
        "              download = True,\n",
        "              transform = transforms.Compose([transforms.ToTensor()])\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "print('Done loading the train set' , train_set)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:02, 12965816.16it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 102869.23it/s]           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 4289058.92it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 32008.82it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Done loading the train set Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Split: train\n",
            "    Root Location: ./data/FashionMNIST\n",
            "    Transforms (if any): Compose(\n",
            "                             ToTensor()\n",
            "                         )\n",
            "    Target Transforms (if any): None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XnARsiatCXN8",
        "colab_type": "code",
        "outputId": "b49624ac-f357-4487-f96c-137672ae0019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1493
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_set[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
            "          0.0000, 0.0000, 0.0510, 0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
            "          0.0039, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n",
            "          0.0000, 0.1412, 0.5333, 0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0157, 0.0000,\n",
            "          0.0000, 0.0118],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,\n",
            "          0.0000, 0.4000, 0.8000, 0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471,\n",
            "          0.0392, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6078, 0.9255, 0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902, 0.3020, 0.5098,\n",
            "          0.2824, 0.0588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
            "          0.2706, 0.8118, 0.8745, 0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725, 0.5529, 0.3451,\n",
            "          0.6745, 0.2588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000,\n",
            "          0.7843, 0.9098, 0.9098, 0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980, 0.4824, 0.7686,\n",
            "          0.8980, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.7176, 0.8824, 0.8471, 0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667, 0.8745, 0.9608,\n",
            "          0.6784, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.7569, 0.8941, 0.8549, 0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745, 0.8627, 0.9529,\n",
            "          0.7922, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0000, 0.0471,\n",
            "          0.8588, 0.8627, 0.8314, 0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314, 0.8863, 0.7725,\n",
            "          0.8196, 0.2039],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.3882,\n",
            "          0.9569, 0.8706, 0.8627, 0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627, 0.9608, 0.4667,\n",
            "          0.6549, 0.2196],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.2157,\n",
            "          0.9255, 0.8941, 0.9020, 0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510, 0.8510, 0.8196,\n",
            "          0.3608, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9294,\n",
            "          0.8863, 0.8510, 0.8745, 0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431, 0.8549, 1.0000,\n",
            "          0.3020, 0.0000],\n",
            "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.5686, 0.8000, 0.8941,\n",
            "          0.8118, 0.8353, 0.8667, 0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431, 0.8784, 0.9569,\n",
            "          0.6235, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196, 0.7412, 0.8941, 0.8627, 0.8706, 0.8510,\n",
            "          0.8863, 0.7843, 0.8039, 0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725, 0.9137, 0.9333,\n",
            "          0.8431, 0.0000],\n",
            "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157, 0.8000, 0.8392, 0.8157, 0.8196, 0.7843,\n",
            "          0.6235, 0.9608, 0.7569, 0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275, 0.8627, 0.9098,\n",
            "          0.9647, 0.0000],\n",
            "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392, 0.8039, 0.8039, 0.8039, 0.8627, 0.9412,\n",
            "          0.3137, 0.5882, 1.0000, 0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196, 0.8706, 0.8941,\n",
            "          0.8824, 0.0000],\n",
            "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176, 0.9765, 0.8627, 0.7608, 0.8431, 0.8510,\n",
            "          0.9451, 0.2549, 0.2863, 0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745, 0.8745, 0.8784,\n",
            "          0.8980, 0.1137],\n",
            "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824, 0.8471, 0.7255, 0.7725, 0.8078, 0.7765,\n",
            "          0.8353, 0.9412, 0.7647, 0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706, 0.8627, 0.8667,\n",
            "          0.9020, 0.2627],\n",
            "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451, 0.7608, 0.7529, 0.7922, 0.8392, 0.8588,\n",
            "          0.8667, 0.8627, 0.9255, 0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745, 0.7098, 0.8039,\n",
            "          0.8078, 0.4510],\n",
            "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686, 0.8000, 0.8235, 0.8353, 0.8118, 0.8275,\n",
            "          0.8235, 0.7843, 0.7686, 0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118, 0.6549, 0.6941,\n",
            "          0.8235, 0.3608],\n",
            "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745, 0.6863, 0.7098, 0.7255, 0.7373, 0.7412,\n",
            "          0.7373, 0.7569, 0.7765, 0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608, 0.7529, 0.8471,\n",
            "          0.6667, 0.0000],\n",
            "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294, 0.9373, 0.9490, 0.9647, 0.9529, 0.9569,\n",
            "          0.8667, 0.8627, 0.7569, 0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588, 0.3882, 0.2275,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.2392, 0.1725, 0.2824, 0.1608, 0.1373,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]]]), 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eWqz8KuJ-rY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loading The Data in a dataloader to acccess shuffling and batch params...\n",
        "#A part of ETL (Extract Transform and Load...)\n",
        "train_loader = torch.utils.data.DataLoader(train_set , batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kY0GKJrRBPHV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Analyzing the dataset...\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_printoptions(linewidth=120)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qijyquPoBYSu",
        "colab_type": "code",
        "outputId": "fb0ab85e-68ac-48a1-8be8-000aca7ac311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#No. of training samples...\n",
        "\n",
        "len(train_set)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "h_wRZbMUD1rT",
        "colab_type": "code",
        "outputId": "ba083c33-a564-41a5-d623-1350779010a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# No of labels in training set as Tensors...\n",
        "train_set.train_labels\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 0, 0,  ..., 3, 0, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "qNncCn4pEBLg",
        "colab_type": "code",
        "outputId": "e889ef4a-2441-4ba7-88fa-5af360078fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "#bin_count() => gives the number of label appearing in the training set..\n",
        "train_set.train_labels.bincount()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "dMxisaXwEdJ-",
        "colab_type": "code",
        "outputId": "b5141806-e7e8-482f-e274-fb4f767fa81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Explanation of output 9 appeared 6000 times and so on..\n",
        "\n",
        "\n",
        "#Itearating over the train_set through iter() which returns a tuple when called by next()\n",
        "\n",
        "\n",
        "sample = next(iter(train_set))\n",
        "print(len(sample) , type(sample))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 <class 'tuple'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LShP7TEjGH8H",
        "colab_type": "code",
        "outputId": "d4e178c3-ae77-440c-e631-89620e4b2323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1008
        }
      },
      "cell_type": "code",
      "source": [
        "print(sample[0][0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
            "         0.0000, 0.0510, 0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000,\n",
            "         0.1412, 0.5333, 0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0157, 0.0000, 0.0000, 0.0118],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000,\n",
            "         0.4000, 0.8000, 0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471, 0.0392, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.6078, 0.9255, 0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902, 0.3020, 0.5098, 0.2824, 0.0588],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706,\n",
            "         0.8118, 0.8745, 0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725, 0.5529, 0.3451, 0.6745, 0.2588],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843,\n",
            "         0.9098, 0.9098, 0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980, 0.4824, 0.7686, 0.8980, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176,\n",
            "         0.8824, 0.8471, 0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667, 0.8745, 0.9608, 0.6784, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569,\n",
            "         0.8941, 0.8549, 0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745, 0.8627, 0.9529, 0.7922, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588,\n",
            "         0.8627, 0.8314, 0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314, 0.8863, 0.7725, 0.8196, 0.2039],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569,\n",
            "         0.8706, 0.8627, 0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627, 0.9608, 0.4667, 0.6549, 0.2196],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255,\n",
            "         0.8941, 0.9020, 0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510, 0.8510, 0.8196, 0.3608, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863,\n",
            "         0.8510, 0.8745, 0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431, 0.8549, 1.0000, 0.3020, 0.0000],\n",
            "        [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118,\n",
            "         0.8353, 0.8667, 0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431, 0.8784, 0.9569, 0.6235, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196, 0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863,\n",
            "         0.7843, 0.8039, 0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725, 0.9137, 0.9333, 0.8431, 0.0000],\n",
            "        [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157, 0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235,\n",
            "         0.9608, 0.7569, 0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275, 0.8627, 0.9098, 0.9647, 0.0000],\n",
            "        [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392, 0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137,\n",
            "         0.5882, 1.0000, 0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196, 0.8706, 0.8941, 0.8824, 0.0000],\n",
            "        [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176, 0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451,\n",
            "         0.2549, 0.2863, 0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745, 0.8745, 0.8784, 0.8980, 0.1137],\n",
            "        [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824, 0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353,\n",
            "         0.9412, 0.7647, 0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706, 0.8627, 0.8667, 0.9020, 0.2627],\n",
            "        [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451, 0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667,\n",
            "         0.8627, 0.9255, 0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745, 0.7098, 0.8039, 0.8078, 0.4510],\n",
            "        [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686, 0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235,\n",
            "         0.7843, 0.7686, 0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118, 0.6549, 0.6941, 0.8235, 0.3608],\n",
            "        [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745, 0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373,\n",
            "         0.7569, 0.7765, 0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608, 0.7529, 0.8471, 0.6667, 0.0000],\n",
            "        [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294, 0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667,\n",
            "         0.8627, 0.7569, 0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588, 0.3882, 0.2275, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "StGB9KcSGi8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image , label = sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Of0xKtveGt-D",
        "colab_type": "code",
        "outputId": "0f6aaec6-ff0d-43cc-c2e9-740612d569f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1493
        }
      },
      "cell_type": "code",
      "source": [
        "print(image)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
            "          0.0000, 0.0000, 0.0510, 0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
            "          0.0039, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n",
            "          0.0000, 0.1412, 0.5333, 0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0157, 0.0000,\n",
            "          0.0000, 0.0118],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,\n",
            "          0.0000, 0.4000, 0.8000, 0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471,\n",
            "          0.0392, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6078, 0.9255, 0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902, 0.3020, 0.5098,\n",
            "          0.2824, 0.0588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
            "          0.2706, 0.8118, 0.8745, 0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725, 0.5529, 0.3451,\n",
            "          0.6745, 0.2588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000,\n",
            "          0.7843, 0.9098, 0.9098, 0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980, 0.4824, 0.7686,\n",
            "          0.8980, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.7176, 0.8824, 0.8471, 0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667, 0.8745, 0.9608,\n",
            "          0.6784, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.7569, 0.8941, 0.8549, 0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745, 0.8627, 0.9529,\n",
            "          0.7922, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0000, 0.0471,\n",
            "          0.8588, 0.8627, 0.8314, 0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314, 0.8863, 0.7725,\n",
            "          0.8196, 0.2039],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.3882,\n",
            "          0.9569, 0.8706, 0.8627, 0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627, 0.9608, 0.4667,\n",
            "          0.6549, 0.2196],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.2157,\n",
            "          0.9255, 0.8941, 0.9020, 0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510, 0.8510, 0.8196,\n",
            "          0.3608, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9294,\n",
            "          0.8863, 0.8510, 0.8745, 0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431, 0.8549, 1.0000,\n",
            "          0.3020, 0.0000],\n",
            "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.5686, 0.8000, 0.8941,\n",
            "          0.8118, 0.8353, 0.8667, 0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431, 0.8784, 0.9569,\n",
            "          0.6235, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196, 0.7412, 0.8941, 0.8627, 0.8706, 0.8510,\n",
            "          0.8863, 0.7843, 0.8039, 0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725, 0.9137, 0.9333,\n",
            "          0.8431, 0.0000],\n",
            "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157, 0.8000, 0.8392, 0.8157, 0.8196, 0.7843,\n",
            "          0.6235, 0.9608, 0.7569, 0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275, 0.8627, 0.9098,\n",
            "          0.9647, 0.0000],\n",
            "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392, 0.8039, 0.8039, 0.8039, 0.8627, 0.9412,\n",
            "          0.3137, 0.5882, 1.0000, 0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196, 0.8706, 0.8941,\n",
            "          0.8824, 0.0000],\n",
            "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176, 0.9765, 0.8627, 0.7608, 0.8431, 0.8510,\n",
            "          0.9451, 0.2549, 0.2863, 0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745, 0.8745, 0.8784,\n",
            "          0.8980, 0.1137],\n",
            "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824, 0.8471, 0.7255, 0.7725, 0.8078, 0.7765,\n",
            "          0.8353, 0.9412, 0.7647, 0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706, 0.8627, 0.8667,\n",
            "          0.9020, 0.2627],\n",
            "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451, 0.7608, 0.7529, 0.7922, 0.8392, 0.8588,\n",
            "          0.8667, 0.8627, 0.9255, 0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745, 0.7098, 0.8039,\n",
            "          0.8078, 0.4510],\n",
            "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686, 0.8000, 0.8235, 0.8353, 0.8118, 0.8275,\n",
            "          0.8235, 0.7843, 0.7686, 0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118, 0.6549, 0.6941,\n",
            "          0.8235, 0.3608],\n",
            "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745, 0.6863, 0.7098, 0.7255, 0.7373, 0.7412,\n",
            "          0.7373, 0.7569, 0.7765, 0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608, 0.7529, 0.8471,\n",
            "          0.6667, 0.0000],\n",
            "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294, 0.9373, 0.9490, 0.9647, 0.9529, 0.9569,\n",
            "          0.8667, 0.8627, 0.7569, 0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588, 0.3882, 0.2275,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.2392, 0.1725, 0.2824, 0.1608, 0.1373,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UDblEyp-Gvdd",
        "colab_type": "code",
        "outputId": "7469f332-3789-4345-a6ce-4bf2f618ca64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "np.shape(image) , np.shape(label)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), ())"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "pit0g0MkHHbV",
        "colab_type": "code",
        "outputId": "a9f9e54a-c667-472c-98bb-d85f1acfa4f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "image = torch.tensor(image , dtype = torch.int32)\n",
        "label = torch.tensor(label , dtype = torch.int32)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "OkksxDHrHeDc",
        "colab_type": "code",
        "outputId": "a235a701-b74a-4b02-8be2-c73bde98c1a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "image.shape , label.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), torch.Size([]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "WW-75t-2H2jc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKq7jIwOIHTk",
        "colab_type": "code",
        "outputId": "22f83006-2d33-4fff-cb63-0139b5a35146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plt.imshow(image.squeeze() , cmap = 'gray')\n",
        "print(image.shape)\n",
        "print('label:' , label)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "label: tensor(9, dtype=torch.int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "NJyCf8JmJPs_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zouuhjXjJrhL",
        "colab_type": "code",
        "outputId": "dcb3ab3f-d06a-4c88-d8d9-058bbefe702c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(batch)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "f5KTpDxaJyd3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Trying Iter function to iterate over a list\n",
        "el = iter([(1,2) , (3,4)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tM3uGgwGJ7aL",
        "colab_type": "code",
        "outputId": "a05c9e78-daea-43aa-d96b-723b25082992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    try:\n",
        "        # get the next item\n",
        "        element = next(el)\n",
        "        print(element)\n",
        "        # do something with element\n",
        "    except StopIteration:\n",
        "        # if StopIteration is raised, break from loop\n",
        "        break"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 2)\n",
            "(3, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wr9zuSgYKEnd",
        "colab_type": "code",
        "outputId": "05860878-1985-4d7c-9beb-8615fa56d351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "cell_type": "code",
      "source": [
        "batch[0]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              "\n",
              "\n",
              "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              "\n",
              "\n",
              "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.7059, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.2824, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.2745, 0.0000, 0.0000]]],\n",
              "\n",
              "\n",
              "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.1529, 0.0039, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.9333, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.5137, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              "\n",
              "\n",
              "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0118, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "74fP1iBvLvFN",
        "colab_type": "code",
        "outputId": "319e7528-3e6c-41f0-eacc-36634e03fb89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(batch[0])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "sMNQ0QYiL8S5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "images, labels = batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HDaOpuowMAFJ",
        "colab_type": "code",
        "outputId": "91d12f94-35fe-4664-d188-4b3f8a21e27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "images.shape , labels.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 1, 28, 28]), torch.Size([10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "jZ8iKXGaMCXz",
        "colab_type": "code",
        "outputId": "290715e1-edf4-4d8e-a1c3-630d7bffa2e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "#Lets see what grid looks like..\n",
        "\n",
        "grid = vision.utils.make_grid(images , nrow = 10 )\n",
        "grid_numpy = grid.numpy()\n",
        "\n",
        "print(grid_numpy.shape)\n",
        "\n",
        "print(np.transpose(grid_numpy, (1, 0, 2)).shape)\n",
        "tp = np.transpose(grid , (1,2,0))\n",
        "plt.imshow(tp)\n",
        "print('labels:' , labels)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 32, 302)\n",
            "(32, 3, 302)\n",
            "labels: tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAABRCAYAAADsHL61AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZYlEIpFIuglSKEskEolE0k34f7S3AB+HNrnyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wq30U9kkP7i2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We create the neural networks by importing nn package\n",
        "\n",
        "#Important things to remember\n",
        "#1.Linear Transformations - Code\n",
        "#2.Weights(Learnable params) - Data\n",
        "\n",
        "\n",
        "\n",
        "#Rule 2:\n",
        "'''\n",
        "The tensor flows first through the forward pass from the \n",
        "layers reaching the output layer , this is known as feed-forward-network\n",
        "\n",
        "#This is done by extending nn.Functional module.\n",
        "\n",
        "#Take-way - Read the classes to understand it in a better sense.\n",
        "\n",
        "'''\n",
        "\n",
        "#A dummy network without extending nn.Module class.\n",
        "\n",
        "\n",
        "\n",
        "class DummyNet:\n",
        "  def __init__(self , layer):\n",
        "    \n",
        "    self.layer = None\n",
        "    \n",
        "  def forward(self, t):\n",
        "    t = self.layer(t)\n",
        "    return t\n",
        "  \n",
        "dumb_net = DummyNet(torch.rand((1,2)))\n",
        "#print(dumb_net.forward())\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4K10M1lFZgFo",
        "colab_type": "code",
        "outputId": "a8e523ce-f97b-4e99-dad5-d93672e2599d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "#Lets implement a neural network which does some good ...\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Functional_Neural_Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    #calling super constructor to keep the track of weights updated in the process(training..)\n",
        "    print('==what===' , super(Functional_Neural_Net , self).__init__())\n",
        "    self.conv1 = nn.Conv2d(in_channels=1 , out_channels=6 , kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6 , out_channels = 12 , kernel_size=5)\n",
        "    #12*4*4 is basically flattening the output before feeding into Fully connected linear layer.\n",
        "    \n",
        "    self.fc1 = nn.Linear(12 * 4 * 4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    \n",
        "Functional_Neural_Net()\n",
        "    \n",
        "    "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==what=== None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Functional_Neural_Net(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "82VwPvn9dWEq",
        "colab_type": "code",
        "outputId": "e06dfd7b-eab2-477e-8c05-d00fd4d02afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#The Updation of weights or parameters happens because of the Module class that inherits the Linear class in python....\n",
        "\n",
        "#Note:- Please see the Module.py class and Linear.py for better understanding....\n",
        "\n",
        "#Majorly there are two params\n",
        "\n",
        "#HyperParameters\n",
        "#Data Dependent Parameters\n",
        "\n",
        "\n",
        "'''\n",
        "Some cool things to remember before building our first NN\n",
        "\n",
        "input_channels = One input channel (Gray scale image ) , alternatively can have 3 in_channels i.e RGB\n",
        "\n",
        "kernel_size = Sets the number of filters with number of output channels.One filter produces one output channel\n",
        "\n",
        "output_features = Number of output tensors\n",
        "\n",
        "\n",
        "\n",
        "#Data dependent parameters = in_channels and out_channels\n",
        "\n",
        "#outputs feature maps = until the network is fed into the fully connected layer  i.e 1 tensor form\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "#12*4*4 comes from the formula [(inputSize + 2*pad - filterSize)/stride] + 1\n",
        "\n",
        "#as we have 2 convolutions ,Therefore formulating as:-\n",
        "\n",
        "\n",
        "# padding = 0 \n",
        "#input_size =  (28*28)\n",
        "#filterSize = 5\n",
        "\n",
        "conv_first_layer = (28-5/1) + 1\n",
        "conv_second_layer =(conv_first_layer - 5 )/1 + 1\n",
        "print(\"Fully connected layer would have dimensions as :===\" , conv_second_layer)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fully connected layer would have dimensions as :=== 20.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4_PdPEoQshj6",
        "colab_type": "code",
        "outputId": "91fcc61d-8c39-472a-f17a-02ceb3adbeb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "#therefore the fully connected layer would have 20*20 pixels if flattened , but as we will be applying max_pool2d layer to the onvolution therefore the \n",
        "#pixels size would reduce to 4*4\n",
        "'''\n",
        "\n",
        "deeplizard\n",
        "Hey again AvZ - I like your question, and I'm glad you took a chance by asking it! You're logic is correct. \n",
        "The reason we are at 4x4 instead of 20x20 after conv2 is due to some details that haven't yet to be presented. \n",
        "In our forward method implementation, we will use some pooling operations that also reduce the dimensions.\n",
        "\n",
        "\n",
        "It will be like this: \n",
        "The output of conv1 should be ((28-5) + 1) / 2 = 12x12 image\n",
        "The output of conv2 should be ((12-5) + 1) / 2 = 4x4 image\n",
        "\n",
        "Learnable Parameters = Which are learned during the training process i.e computing and minimizing the loss function\n",
        "\n",
        "If we dont specify the super() thing in our code , we will get simply just the object and nothing else...\n",
        "\n",
        "Module class (In a nutshell) :  The Module class contains Layers , therefore when we call the super constructor we refer to the Layer class which inded creates Layers.\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "fNet = Functional_Neural_Net()\n",
        "\n",
        "print(fNet)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==what=== None\n",
            "Functional_Neural_Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tk8D9gIhkIUL",
        "colab_type": "code",
        "outputId": "9a8dfd68-4a0e-4c81-e78d-1b32703e331d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "fNet.conv1"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "SSYhWAQckVPe",
        "colab_type": "code",
        "outputId": "46d4a307-fd6e-410b-bcc5-d12846b3455e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "cell_type": "code",
      "source": [
        "#our convolution layer contains weight object (lies in conv1 layer) \n",
        "fNet.conv1.weight\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.1985, -0.1159,  0.1639,  0.0378,  0.1709],\n",
              "          [ 0.0689, -0.0647,  0.0894,  0.0132, -0.1996],\n",
              "          [ 0.0918, -0.1420,  0.1636, -0.0206, -0.0385],\n",
              "          [-0.1301,  0.0539, -0.1110,  0.1889, -0.1616],\n",
              "          [-0.0236,  0.0883,  0.1814,  0.1380,  0.0432]]],\n",
              "\n",
              "\n",
              "        [[[-0.1265,  0.1817, -0.0211,  0.0190, -0.0212],\n",
              "          [ 0.1104, -0.1023,  0.1608,  0.0095, -0.0302],\n",
              "          [ 0.1460, -0.1132, -0.0804,  0.1267, -0.1857],\n",
              "          [-0.1280, -0.0782, -0.0367, -0.1502,  0.0398],\n",
              "          [-0.1016,  0.1995, -0.0940, -0.1445, -0.0855]]],\n",
              "\n",
              "\n",
              "        [[[-0.1658, -0.0253, -0.1938,  0.0210,  0.1190],\n",
              "          [ 0.0305,  0.1550, -0.0147, -0.1948, -0.1619],\n",
              "          [ 0.1175, -0.1711,  0.1211, -0.0705,  0.1882],\n",
              "          [-0.0893, -0.0702, -0.1216, -0.1865, -0.1998],\n",
              "          [ 0.1368,  0.1173,  0.1249, -0.1753,  0.1471]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1958, -0.0031, -0.1857,  0.0419, -0.0133],\n",
              "          [-0.0532,  0.1794,  0.1948,  0.1104, -0.1156],\n",
              "          [ 0.1324,  0.1909, -0.1808,  0.1248, -0.1984],\n",
              "          [ 0.0211, -0.0168, -0.0551,  0.0174, -0.0261],\n",
              "          [-0.0070, -0.0630, -0.1644,  0.1110,  0.1871]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1837,  0.0704, -0.1990,  0.0646,  0.1866],\n",
              "          [ 0.1451, -0.1500,  0.0377,  0.0624, -0.0015],\n",
              "          [ 0.1854,  0.1725, -0.0074, -0.0312, -0.0218],\n",
              "          [-0.0629,  0.0055,  0.0076, -0.0945,  0.1358],\n",
              "          [ 0.0980, -0.0476, -0.0432, -0.1671, -0.1231]]],\n",
              "\n",
              "\n",
              "        [[[-0.0761,  0.1220, -0.1624,  0.1248,  0.1004],\n",
              "          [-0.0834,  0.0331,  0.1076, -0.1536, -0.1589],\n",
              "          [ 0.1198,  0.1739,  0.0490,  0.1876,  0.0722],\n",
              "          [ 0.1671, -0.0465,  0.0975,  0.0061, -0.1215],\n",
              "          [-0.1307, -0.0506,  0.1668,  0.1173, -0.1546]]]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "ptOcL8oQkvvS",
        "colab_type": "code",
        "outputId": "ecb918bd-cf8b-47e1-b4ab-44ade0d2516b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Parameter containing:\n",
        "\n",
        "#Parameter extends torch.Tensor class , therefore weight extends Parameter class and when we specify .weight, it return the tensor as weight parameters...\n",
        "\n",
        "fNet.conv1.weight.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 1, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "-O5xJPrPE8DD",
        "colab_type": "code",
        "outputId": "58e78b79-1e8e-43d4-e145-e21a44f590c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Result of the first outp[ut says that  we have 6 filters (feature maps) , 1 input_channel (Gray scale image) , kernel_size(5,5)\n",
        "\n",
        "\n",
        "fNet.conv2.weight.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 6, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "BfIXIvzmFQvk",
        "colab_type": "code",
        "outputId": "f0d32e48-4b3f-4471-c0e1-ecfbe054b9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#The second results that we have 12 filters (coming from the first input channel , 6 => input channels which are returned from the output of first conv1 layer\n",
        "fNet.fc1.weight.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([120, 192])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "kt_mWcAyFudD",
        "colab_type": "code",
        "outputId": "c0fe8e23-05e6-4ded-9c73-8ade8130b8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#120 => parameters returned from conv 2 layer ,  filters = 192\n",
        "\n",
        "fNet.parameters()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f43b550eaf0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "AQch3e3YJlxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for name,param in fNet.named_parameters():\n",
        "  print(name + '\\t' +str(param.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fYlkCQw4J1fj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Training our first NN model...\n",
        "print(train_set[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scxauj0bK18C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GPLxydgQYkpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # to handle matrix and data operation\n",
        "import pandas as pd # to read csv and handle dataframe\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRHAnhDTbwQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "dd9e1f18-4319-47f4-c682-ac03f4dc2d83"
      },
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "    self.fMnistConv1 = nn.Conv2d(in_channels = 1 , out_channels = 6 , kernel_size = 5)\n",
        "    self.fMnistConv2 = nn.Conv2d(in_channels = 6 , out_channels = 12 , kernel_size = 5)\n",
        "\n",
        "    #adding fully connected layers\n",
        "\n",
        "    self.fc1 = nn.Linear(12*4*4 , 120)\n",
        "    self.fc2 = nn.Linear(120 , 84)\n",
        "    self.fc3 = nn.Linear(84 , 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Max pooling over a (2, 2) window\n",
        "    x = F.max_pool2d(F.relu(self.fMnistConv1(x)), (2, 2))\n",
        "    # If the size is a square you can only specify a single number\n",
        "    x = F.max_pool2d(F.relu(self.fMnistConv2(x)), 2)\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "  \n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features\n",
        "\n",
        "  \n",
        "  \n",
        "mlp = MLP()\n",
        "print(mlp)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fMnistConv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fMnistConv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5_JuA9x9Xvol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10\n",
        "def fit(model, train_loader):\n",
        "    optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
        "  '''Parameters :-\n",
        "      conv1.weight\ttorch.Size([6, 1, 5, 5])\n",
        "      conv1.bias\ttorch.Size([6])\n",
        "      conv2.weight\ttorch.Size([12, 6, 5, 5])\n",
        "      conv2.bias\ttorch.Size([12])\n",
        "      fc1.weight\ttorch.Size([120, 192])\n",
        "      fc1.bias\ttorch.Size([120])\n",
        "      fc2.weight\ttorch.Size([84, 120])\n",
        "      fc2.bias\ttorch.Size([84])\n",
        "      fc3.weight\ttorch.Size([10, 84])\n",
        "      fc3.bias\ttorch.Size([10])\n",
        "  '''  \n",
        "    error = nn.CrossEntropyLoss()\n",
        "    EPOCHS = 5\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        correct = 0\n",
        "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            var_X_batch = Variable(X_batch).float()\n",
        "            var_y_batch = Variable(y_batch)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(var_X_batch)\n",
        "            loss = error(output, var_y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Total correct predictions\n",
        "            predicted = torch.max(output.data, 1)[1] \n",
        "            #print('====predicted' , predicted)\n",
        "            correct += (predicted == var_y_batch).sum()\n",
        "            #print('Correct' , correct)\n",
        "            if batch_idx % 50 == 0:\n",
        "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
        "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ImlgxYwPYabN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10417
        },
        "outputId": "acaccb93-b4b5-4a31-e01a-c4903fec8f81"
      },
      "cell_type": "code",
      "source": [
        "fit(mlp, train_loader)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 [0/60000 (0%)]\tLoss: 0.376526\t Accuracy:90.000%\n",
            "Epoch : 0 [500/60000 (1%)]\tLoss: 1.451877\t Accuracy:90.392%\n",
            "Epoch : 0 [1000/60000 (2%)]\tLoss: 0.437374\t Accuracy:90.099%\n",
            "Epoch : 0 [1500/60000 (2%)]\tLoss: 0.111584\t Accuracy:89.404%\n",
            "Epoch : 0 [2000/60000 (3%)]\tLoss: 0.209597\t Accuracy:89.303%\n",
            "Epoch : 0 [2500/60000 (4%)]\tLoss: 0.564388\t Accuracy:89.163%\n",
            "Epoch : 0 [3000/60000 (5%)]\tLoss: 0.431075\t Accuracy:88.704%\n",
            "Epoch : 0 [3500/60000 (6%)]\tLoss: 0.592183\t Accuracy:88.718%\n",
            "Epoch : 0 [4000/60000 (7%)]\tLoss: 0.159937\t Accuracy:88.753%\n",
            "Epoch : 0 [4500/60000 (8%)]\tLoss: 0.861704\t Accuracy:88.714%\n",
            "Epoch : 0 [5000/60000 (8%)]\tLoss: 0.111815\t Accuracy:89.002%\n",
            "Epoch : 0 [5500/60000 (9%)]\tLoss: 0.558593\t Accuracy:88.657%\n",
            "Epoch : 0 [6000/60000 (10%)]\tLoss: 0.513477\t Accuracy:88.885%\n",
            "Epoch : 0 [6500/60000 (11%)]\tLoss: 0.435762\t Accuracy:88.986%\n",
            "Epoch : 0 [7000/60000 (12%)]\tLoss: 0.498353\t Accuracy:89.058%\n",
            "Epoch : 0 [7500/60000 (12%)]\tLoss: 0.395784\t Accuracy:89.068%\n",
            "Epoch : 0 [8000/60000 (13%)]\tLoss: 0.344960\t Accuracy:88.964%\n",
            "Epoch : 0 [8500/60000 (14%)]\tLoss: 0.169014\t Accuracy:88.966%\n",
            "Epoch : 0 [9000/60000 (15%)]\tLoss: 0.283070\t Accuracy:88.835%\n",
            "Epoch : 0 [9500/60000 (16%)]\tLoss: 0.235634\t Accuracy:88.875%\n",
            "Epoch : 0 [10000/60000 (17%)]\tLoss: 0.116853\t Accuracy:88.871%\n",
            "Epoch : 0 [10500/60000 (18%)]\tLoss: 0.007246\t Accuracy:89.039%\n",
            "Epoch : 0 [11000/60000 (18%)]\tLoss: 0.455923\t Accuracy:89.037%\n",
            "Epoch : 0 [11500/60000 (19%)]\tLoss: 0.238582\t Accuracy:89.131%\n",
            "Epoch : 0 [12000/60000 (20%)]\tLoss: 0.374402\t Accuracy:88.984%\n",
            "Epoch : 0 [12500/60000 (21%)]\tLoss: 0.200460\t Accuracy:88.993%\n",
            "Epoch : 0 [13000/60000 (22%)]\tLoss: 0.158329\t Accuracy:89.032%\n",
            "Epoch : 0 [13500/60000 (22%)]\tLoss: 0.102992\t Accuracy:88.993%\n",
            "Epoch : 0 [14000/60000 (23%)]\tLoss: 0.046055\t Accuracy:88.936%\n",
            "Epoch : 0 [14500/60000 (24%)]\tLoss: 0.133335\t Accuracy:88.946%\n",
            "Epoch : 0 [15000/60000 (25%)]\tLoss: 0.136489\t Accuracy:89.027%\n",
            "Epoch : 0 [15500/60000 (26%)]\tLoss: 0.191546\t Accuracy:89.046%\n",
            "Epoch : 0 [16000/60000 (27%)]\tLoss: 1.029246\t Accuracy:89.082%\n",
            "Epoch : 0 [16500/60000 (28%)]\tLoss: 0.241529\t Accuracy:89.067%\n",
            "Epoch : 0 [17000/60000 (28%)]\tLoss: 0.073358\t Accuracy:89.024%\n",
            "Epoch : 0 [17500/60000 (29%)]\tLoss: 0.218445\t Accuracy:89.001%\n",
            "Epoch : 0 [18000/60000 (30%)]\tLoss: 0.944725\t Accuracy:88.978%\n",
            "Epoch : 0 [18500/60000 (31%)]\tLoss: 0.111932\t Accuracy:88.968%\n",
            "Epoch : 0 [19000/60000 (32%)]\tLoss: 0.735816\t Accuracy:89.011%\n",
            "Epoch : 0 [19500/60000 (32%)]\tLoss: 0.074350\t Accuracy:88.990%\n",
            "Epoch : 0 [20000/60000 (33%)]\tLoss: 0.402228\t Accuracy:88.971%\n",
            "Epoch : 0 [20500/60000 (34%)]\tLoss: 0.033024\t Accuracy:88.976%\n",
            "Epoch : 0 [21000/60000 (35%)]\tLoss: 0.077360\t Accuracy:89.005%\n",
            "Epoch : 0 [21500/60000 (36%)]\tLoss: 0.275695\t Accuracy:88.945%\n",
            "Epoch : 0 [22000/60000 (37%)]\tLoss: 0.016008\t Accuracy:88.996%\n",
            "Epoch : 0 [22500/60000 (38%)]\tLoss: 0.561868\t Accuracy:88.987%\n",
            "Epoch : 0 [23000/60000 (38%)]\tLoss: 0.131733\t Accuracy:88.983%\n",
            "Epoch : 0 [23500/60000 (39%)]\tLoss: 0.164454\t Accuracy:88.966%\n",
            "Epoch : 0 [24000/60000 (40%)]\tLoss: 0.531851\t Accuracy:89.025%\n",
            "Epoch : 0 [24500/60000 (41%)]\tLoss: 0.386730\t Accuracy:89.021%\n",
            "Epoch : 0 [25000/60000 (42%)]\tLoss: 0.051508\t Accuracy:88.996%\n",
            "Epoch : 0 [25500/60000 (42%)]\tLoss: 0.513524\t Accuracy:89.036%\n",
            "Epoch : 0 [26000/60000 (43%)]\tLoss: 0.144085\t Accuracy:89.020%\n",
            "Epoch : 0 [26500/60000 (44%)]\tLoss: 0.166060\t Accuracy:88.970%\n",
            "Epoch : 0 [27000/60000 (45%)]\tLoss: 0.012367\t Accuracy:88.982%\n",
            "Epoch : 0 [27500/60000 (46%)]\tLoss: 0.341215\t Accuracy:89.011%\n",
            "Epoch : 0 [28000/60000 (47%)]\tLoss: 0.769126\t Accuracy:89.011%\n",
            "Epoch : 0 [28500/60000 (48%)]\tLoss: 0.372265\t Accuracy:89.000%\n",
            "Epoch : 0 [29000/60000 (48%)]\tLoss: 0.063685\t Accuracy:89.004%\n",
            "Epoch : 0 [29500/60000 (49%)]\tLoss: 0.370646\t Accuracy:88.994%\n",
            "Epoch : 0 [30000/60000 (50%)]\tLoss: 0.011295\t Accuracy:88.960%\n",
            "Epoch : 0 [30500/60000 (51%)]\tLoss: 0.422724\t Accuracy:88.938%\n",
            "Epoch : 0 [31000/60000 (52%)]\tLoss: 0.080472\t Accuracy:88.939%\n",
            "Epoch : 0 [31500/60000 (52%)]\tLoss: 0.321184\t Accuracy:88.899%\n",
            "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.505090\t Accuracy:88.925%\n",
            "Epoch : 0 [32500/60000 (54%)]\tLoss: 0.188441\t Accuracy:88.911%\n",
            "Epoch : 0 [33000/60000 (55%)]\tLoss: 0.095393\t Accuracy:88.912%\n",
            "Epoch : 0 [33500/60000 (56%)]\tLoss: 0.926353\t Accuracy:88.932%\n",
            "Epoch : 0 [34000/60000 (57%)]\tLoss: 0.504332\t Accuracy:88.897%\n",
            "Epoch : 0 [34500/60000 (58%)]\tLoss: 0.152529\t Accuracy:88.899%\n",
            "Epoch : 0 [35000/60000 (58%)]\tLoss: 0.464372\t Accuracy:88.903%\n",
            "Epoch : 0 [35500/60000 (59%)]\tLoss: 0.755748\t Accuracy:88.905%\n",
            "Epoch : 0 [36000/60000 (60%)]\tLoss: 0.041142\t Accuracy:88.898%\n",
            "Epoch : 0 [36500/60000 (61%)]\tLoss: 0.381839\t Accuracy:88.885%\n",
            "Epoch : 0 [37000/60000 (62%)]\tLoss: 0.429716\t Accuracy:88.860%\n",
            "Epoch : 0 [37500/60000 (62%)]\tLoss: 0.285839\t Accuracy:88.864%\n",
            "Epoch : 0 [38000/60000 (63%)]\tLoss: 0.484626\t Accuracy:88.885%\n",
            "Epoch : 0 [38500/60000 (64%)]\tLoss: 0.257887\t Accuracy:88.920%\n",
            "Epoch : 0 [39000/60000 (65%)]\tLoss: 0.109340\t Accuracy:88.923%\n",
            "Epoch : 0 [39500/60000 (66%)]\tLoss: 0.145983\t Accuracy:88.932%\n",
            "Epoch : 0 [40000/60000 (67%)]\tLoss: 0.247744\t Accuracy:88.938%\n",
            "Epoch : 0 [40500/60000 (68%)]\tLoss: 0.308050\t Accuracy:88.921%\n",
            "Epoch : 0 [41000/60000 (68%)]\tLoss: 0.128381\t Accuracy:88.895%\n",
            "Epoch : 0 [41500/60000 (69%)]\tLoss: 0.312598\t Accuracy:88.875%\n",
            "Epoch : 0 [42000/60000 (70%)]\tLoss: 0.115789\t Accuracy:88.865%\n",
            "Epoch : 0 [42500/60000 (71%)]\tLoss: 0.211651\t Accuracy:88.859%\n",
            "Epoch : 0 [43000/60000 (72%)]\tLoss: 0.329482\t Accuracy:88.872%\n",
            "Epoch : 0 [43500/60000 (72%)]\tLoss: 0.329939\t Accuracy:88.867%\n",
            "Epoch : 0 [44000/60000 (73%)]\tLoss: 0.141980\t Accuracy:88.889%\n",
            "Epoch : 0 [44500/60000 (74%)]\tLoss: 0.375438\t Accuracy:88.877%\n",
            "Epoch : 0 [45000/60000 (75%)]\tLoss: 0.435640\t Accuracy:88.860%\n",
            "Epoch : 0 [45500/60000 (76%)]\tLoss: 0.237892\t Accuracy:88.831%\n",
            "Epoch : 0 [46000/60000 (77%)]\tLoss: 0.289312\t Accuracy:88.831%\n",
            "Epoch : 0 [46500/60000 (78%)]\tLoss: 0.265671\t Accuracy:88.826%\n",
            "Epoch : 0 [47000/60000 (78%)]\tLoss: 0.085131\t Accuracy:88.834%\n",
            "Epoch : 0 [47500/60000 (79%)]\tLoss: 0.954127\t Accuracy:88.815%\n",
            "Epoch : 0 [48000/60000 (80%)]\tLoss: 0.247411\t Accuracy:88.819%\n",
            "Epoch : 0 [48500/60000 (81%)]\tLoss: 0.042715\t Accuracy:88.833%\n",
            "Epoch : 0 [49000/60000 (82%)]\tLoss: 0.480684\t Accuracy:88.829%\n",
            "Epoch : 0 [49500/60000 (82%)]\tLoss: 0.774610\t Accuracy:88.820%\n",
            "Epoch : 0 [50000/60000 (83%)]\tLoss: 0.021554\t Accuracy:88.824%\n",
            "Epoch : 0 [50500/60000 (84%)]\tLoss: 0.127875\t Accuracy:88.830%\n",
            "Epoch : 0 [51000/60000 (85%)]\tLoss: 0.463576\t Accuracy:88.824%\n",
            "Epoch : 0 [51500/60000 (86%)]\tLoss: 0.055730\t Accuracy:88.833%\n",
            "Epoch : 0 [52000/60000 (87%)]\tLoss: 0.174449\t Accuracy:88.833%\n",
            "Epoch : 0 [52500/60000 (88%)]\tLoss: 0.747516\t Accuracy:88.844%\n",
            "Epoch : 0 [53000/60000 (88%)]\tLoss: 0.607109\t Accuracy:88.832%\n",
            "Epoch : 0 [53500/60000 (89%)]\tLoss: 0.411500\t Accuracy:88.826%\n",
            "Epoch : 0 [54000/60000 (90%)]\tLoss: 0.094390\t Accuracy:88.832%\n",
            "Epoch : 0 [54500/60000 (91%)]\tLoss: 0.247478\t Accuracy:88.809%\n",
            "Epoch : 0 [55000/60000 (92%)]\tLoss: 0.206223\t Accuracy:88.806%\n",
            "Epoch : 0 [55500/60000 (92%)]\tLoss: 0.339613\t Accuracy:88.807%\n",
            "Epoch : 0 [56000/60000 (93%)]\tLoss: 0.120079\t Accuracy:88.814%\n",
            "Epoch : 0 [56500/60000 (94%)]\tLoss: 0.340328\t Accuracy:88.816%\n",
            "Epoch : 0 [57000/60000 (95%)]\tLoss: 0.217727\t Accuracy:88.812%\n",
            "Epoch : 0 [57500/60000 (96%)]\tLoss: 0.990331\t Accuracy:88.807%\n",
            "Epoch : 0 [58000/60000 (97%)]\tLoss: 0.379791\t Accuracy:88.819%\n",
            "Epoch : 0 [58500/60000 (98%)]\tLoss: 0.060577\t Accuracy:88.812%\n",
            "Epoch : 0 [59000/60000 (98%)]\tLoss: 0.335466\t Accuracy:88.800%\n",
            "Epoch : 0 [59500/60000 (99%)]\tLoss: 0.305211\t Accuracy:88.807%\n",
            "Epoch : 1 [0/60000 (0%)]\tLoss: 0.309142\t Accuracy:80.000%\n",
            "Epoch : 1 [500/60000 (1%)]\tLoss: 1.119698\t Accuracy:90.196%\n",
            "Epoch : 1 [1000/60000 (2%)]\tLoss: 0.391397\t Accuracy:89.406%\n",
            "Epoch : 1 [1500/60000 (2%)]\tLoss: 0.094917\t Accuracy:89.139%\n",
            "Epoch : 1 [2000/60000 (3%)]\tLoss: 0.127848\t Accuracy:89.204%\n",
            "Epoch : 1 [2500/60000 (4%)]\tLoss: 0.523949\t Accuracy:89.203%\n",
            "Epoch : 1 [3000/60000 (5%)]\tLoss: 0.400403\t Accuracy:89.003%\n",
            "Epoch : 1 [3500/60000 (6%)]\tLoss: 0.499624\t Accuracy:89.003%\n",
            "Epoch : 1 [4000/60000 (7%)]\tLoss: 0.142890\t Accuracy:89.052%\n",
            "Epoch : 1 [4500/60000 (8%)]\tLoss: 0.674517\t Accuracy:88.980%\n",
            "Epoch : 1 [5000/60000 (8%)]\tLoss: 0.182935\t Accuracy:89.261%\n",
            "Epoch : 1 [5500/60000 (9%)]\tLoss: 0.251648\t Accuracy:89.002%\n",
            "Epoch : 1 [6000/60000 (10%)]\tLoss: 0.553865\t Accuracy:89.101%\n",
            "Epoch : 1 [6500/60000 (11%)]\tLoss: 0.178439\t Accuracy:89.171%\n",
            "Epoch : 1 [7000/60000 (12%)]\tLoss: 0.404182\t Accuracy:89.272%\n",
            "Epoch : 1 [7500/60000 (12%)]\tLoss: 0.328702\t Accuracy:89.241%\n",
            "Epoch : 1 [8000/60000 (13%)]\tLoss: 0.341659\t Accuracy:89.189%\n",
            "Epoch : 1 [8500/60000 (14%)]\tLoss: 0.169594\t Accuracy:89.248%\n",
            "Epoch : 1 [9000/60000 (15%)]\tLoss: 0.226544\t Accuracy:89.168%\n",
            "Epoch : 1 [9500/60000 (16%)]\tLoss: 0.271850\t Accuracy:89.222%\n",
            "Epoch : 1 [10000/60000 (17%)]\tLoss: 0.169983\t Accuracy:89.251%\n",
            "Epoch : 1 [10500/60000 (18%)]\tLoss: 0.011230\t Accuracy:89.382%\n",
            "Epoch : 1 [11000/60000 (18%)]\tLoss: 0.207463\t Accuracy:89.373%\n",
            "Epoch : 1 [11500/60000 (19%)]\tLoss: 0.218158\t Accuracy:89.505%\n",
            "Epoch : 1 [12000/60000 (20%)]\tLoss: 0.324456\t Accuracy:89.367%\n",
            "Epoch : 1 [12500/60000 (21%)]\tLoss: 0.250828\t Accuracy:89.392%\n",
            "Epoch : 1 [13000/60000 (22%)]\tLoss: 0.137001\t Accuracy:89.408%\n",
            "Epoch : 1 [13500/60000 (22%)]\tLoss: 0.058797\t Accuracy:89.341%\n",
            "Epoch : 1 [14000/60000 (23%)]\tLoss: 0.058843\t Accuracy:89.315%\n",
            "Epoch : 1 [14500/60000 (24%)]\tLoss: 0.199523\t Accuracy:89.304%\n",
            "Epoch : 1 [15000/60000 (25%)]\tLoss: 0.059363\t Accuracy:89.414%\n",
            "Epoch : 1 [15500/60000 (26%)]\tLoss: 0.222602\t Accuracy:89.375%\n",
            "Epoch : 1 [16000/60000 (27%)]\tLoss: 1.049944\t Accuracy:89.407%\n",
            "Epoch : 1 [16500/60000 (28%)]\tLoss: 0.304238\t Accuracy:89.412%\n",
            "Epoch : 1 [17000/60000 (28%)]\tLoss: 0.124576\t Accuracy:89.394%\n",
            "Epoch : 1 [17500/60000 (29%)]\tLoss: 0.199754\t Accuracy:89.400%\n",
            "Epoch : 1 [18000/60000 (30%)]\tLoss: 0.890776\t Accuracy:89.384%\n",
            "Epoch : 1 [18500/60000 (31%)]\tLoss: 0.087195\t Accuracy:89.384%\n",
            "Epoch : 1 [19000/60000 (32%)]\tLoss: 0.654442\t Accuracy:89.411%\n",
            "Epoch : 1 [19500/60000 (32%)]\tLoss: 0.070153\t Accuracy:89.416%\n",
            "Epoch : 1 [20000/60000 (33%)]\tLoss: 0.291049\t Accuracy:89.405%\n",
            "Epoch : 1 [20500/60000 (34%)]\tLoss: 0.034954\t Accuracy:89.391%\n",
            "Epoch : 1 [21000/60000 (35%)]\tLoss: 0.059707\t Accuracy:89.405%\n",
            "Epoch : 1 [21500/60000 (36%)]\tLoss: 0.329860\t Accuracy:89.354%\n",
            "Epoch : 1 [22000/60000 (37%)]\tLoss: 0.022235\t Accuracy:89.387%\n",
            "Epoch : 1 [22500/60000 (38%)]\tLoss: 0.625198\t Accuracy:89.387%\n",
            "Epoch : 1 [23000/60000 (38%)]\tLoss: 0.135640\t Accuracy:89.379%\n",
            "Epoch : 1 [23500/60000 (39%)]\tLoss: 0.176742\t Accuracy:89.370%\n",
            "Epoch : 1 [24000/60000 (40%)]\tLoss: 0.547878\t Accuracy:89.429%\n",
            "Epoch : 1 [24500/60000 (41%)]\tLoss: 0.286286\t Accuracy:89.445%\n",
            "Epoch : 1 [25000/60000 (42%)]\tLoss: 0.062171\t Accuracy:89.440%\n",
            "Epoch : 1 [25500/60000 (42%)]\tLoss: 0.451708\t Accuracy:89.494%\n",
            "Epoch : 1 [26000/60000 (43%)]\tLoss: 0.137681\t Accuracy:89.473%\n",
            "Epoch : 1 [26500/60000 (44%)]\tLoss: 0.108551\t Accuracy:89.445%\n",
            "Epoch : 1 [27000/60000 (45%)]\tLoss: 0.010649\t Accuracy:89.471%\n",
            "Epoch : 1 [27500/60000 (46%)]\tLoss: 0.331909\t Accuracy:89.487%\n",
            "Epoch : 1 [28000/60000 (47%)]\tLoss: 0.801890\t Accuracy:89.497%\n",
            "Epoch : 1 [28500/60000 (48%)]\tLoss: 0.291448\t Accuracy:89.516%\n",
            "Epoch : 1 [29000/60000 (48%)]\tLoss: 0.040841\t Accuracy:89.524%\n",
            "Epoch : 1 [29500/60000 (49%)]\tLoss: 0.253843\t Accuracy:89.529%\n",
            "Epoch : 1 [30000/60000 (50%)]\tLoss: 0.005739\t Accuracy:89.500%\n",
            "Epoch : 1 [30500/60000 (51%)]\tLoss: 0.356836\t Accuracy:89.492%\n",
            "Epoch : 1 [31000/60000 (52%)]\tLoss: 0.054789\t Accuracy:89.490%\n",
            "Epoch : 1 [31500/60000 (52%)]\tLoss: 0.281190\t Accuracy:89.460%\n",
            "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.578108\t Accuracy:89.494%\n",
            "Epoch : 1 [32500/60000 (54%)]\tLoss: 0.122974\t Accuracy:89.480%\n",
            "Epoch : 1 [33000/60000 (55%)]\tLoss: 0.099613\t Accuracy:89.494%\n",
            "Epoch : 1 [33500/60000 (56%)]\tLoss: 0.949308\t Accuracy:89.520%\n",
            "Epoch : 1 [34000/60000 (57%)]\tLoss: 0.547682\t Accuracy:89.477%\n",
            "Epoch : 1 [34500/60000 (58%)]\tLoss: 0.211827\t Accuracy:89.496%\n",
            "Epoch : 1 [35000/60000 (58%)]\tLoss: 0.363941\t Accuracy:89.509%\n",
            "Epoch : 1 [35500/60000 (59%)]\tLoss: 0.805872\t Accuracy:89.513%\n",
            "Epoch : 1 [36000/60000 (60%)]\tLoss: 0.032900\t Accuracy:89.517%\n",
            "Epoch : 1 [36500/60000 (61%)]\tLoss: 0.351755\t Accuracy:89.474%\n",
            "Epoch : 1 [37000/60000 (62%)]\tLoss: 0.499711\t Accuracy:89.479%\n",
            "Epoch : 1 [37500/60000 (62%)]\tLoss: 0.313622\t Accuracy:89.480%\n",
            "Epoch : 1 [38000/60000 (63%)]\tLoss: 0.417460\t Accuracy:89.511%\n",
            "Epoch : 1 [38500/60000 (64%)]\tLoss: 0.341353\t Accuracy:89.551%\n",
            "Epoch : 1 [39000/60000 (65%)]\tLoss: 0.084204\t Accuracy:89.562%\n",
            "Epoch : 1 [39500/60000 (66%)]\tLoss: 0.128432\t Accuracy:89.549%\n",
            "Epoch : 1 [40000/60000 (67%)]\tLoss: 0.261155\t Accuracy:89.548%\n",
            "Epoch : 1 [40500/60000 (68%)]\tLoss: 0.262874\t Accuracy:89.529%\n",
            "Epoch : 1 [41000/60000 (68%)]\tLoss: 0.144682\t Accuracy:89.525%\n",
            "Epoch : 1 [41500/60000 (69%)]\tLoss: 0.304380\t Accuracy:89.509%\n",
            "Epoch : 1 [42000/60000 (70%)]\tLoss: 0.106669\t Accuracy:89.505%\n",
            "Epoch : 1 [42500/60000 (71%)]\tLoss: 0.319379\t Accuracy:89.490%\n",
            "Epoch : 1 [43000/60000 (72%)]\tLoss: 0.267477\t Accuracy:89.505%\n",
            "Epoch : 1 [43500/60000 (72%)]\tLoss: 0.427904\t Accuracy:89.515%\n",
            "Epoch : 1 [44000/60000 (73%)]\tLoss: 0.125950\t Accuracy:89.541%\n",
            "Epoch : 1 [44500/60000 (74%)]\tLoss: 0.421222\t Accuracy:89.535%\n",
            "Epoch : 1 [45000/60000 (75%)]\tLoss: 0.454964\t Accuracy:89.522%\n",
            "Epoch : 1 [45500/60000 (76%)]\tLoss: 0.152740\t Accuracy:89.508%\n",
            "Epoch : 1 [46000/60000 (77%)]\tLoss: 0.302442\t Accuracy:89.504%\n",
            "Epoch : 1 [46500/60000 (78%)]\tLoss: 0.303302\t Accuracy:89.505%\n",
            "Epoch : 1 [47000/60000 (78%)]\tLoss: 0.105299\t Accuracy:89.519%\n",
            "Epoch : 1 [47500/60000 (79%)]\tLoss: 0.860672\t Accuracy:89.491%\n",
            "Epoch : 1 [48000/60000 (80%)]\tLoss: 0.128665\t Accuracy:89.496%\n",
            "Epoch : 1 [48500/60000 (81%)]\tLoss: 0.069343\t Accuracy:89.493%\n",
            "Epoch : 1 [49000/60000 (82%)]\tLoss: 0.364888\t Accuracy:89.494%\n",
            "Epoch : 1 [49500/60000 (82%)]\tLoss: 0.675476\t Accuracy:89.487%\n",
            "Epoch : 1 [50000/60000 (83%)]\tLoss: 0.025546\t Accuracy:89.490%\n",
            "Epoch : 1 [50500/60000 (84%)]\tLoss: 0.168572\t Accuracy:89.487%\n",
            "Epoch : 1 [51000/60000 (85%)]\tLoss: 0.418928\t Accuracy:89.488%\n",
            "Epoch : 1 [51500/60000 (86%)]\tLoss: 0.066619\t Accuracy:89.493%\n",
            "Epoch : 1 [52000/60000 (87%)]\tLoss: 0.135709\t Accuracy:89.490%\n",
            "Epoch : 1 [52500/60000 (88%)]\tLoss: 0.641682\t Accuracy:89.493%\n",
            "Epoch : 1 [53000/60000 (88%)]\tLoss: 0.548966\t Accuracy:89.474%\n",
            "Epoch : 1 [53500/60000 (89%)]\tLoss: 0.383980\t Accuracy:89.482%\n",
            "Epoch : 1 [54000/60000 (90%)]\tLoss: 0.080952\t Accuracy:89.482%\n",
            "Epoch : 1 [54500/60000 (91%)]\tLoss: 0.290273\t Accuracy:89.470%\n",
            "Epoch : 1 [55000/60000 (92%)]\tLoss: 0.192149\t Accuracy:89.473%\n",
            "Epoch : 1 [55500/60000 (92%)]\tLoss: 0.158252\t Accuracy:89.481%\n",
            "Epoch : 1 [56000/60000 (93%)]\tLoss: 0.081104\t Accuracy:89.472%\n",
            "Epoch : 1 [56500/60000 (94%)]\tLoss: 0.319444\t Accuracy:89.460%\n",
            "Epoch : 1 [57000/60000 (95%)]\tLoss: 0.219758\t Accuracy:89.453%\n",
            "Epoch : 1 [57500/60000 (96%)]\tLoss: 0.822897\t Accuracy:89.459%\n",
            "Epoch : 1 [58000/60000 (97%)]\tLoss: 0.300559\t Accuracy:89.467%\n",
            "Epoch : 1 [58500/60000 (98%)]\tLoss: 0.103366\t Accuracy:89.453%\n",
            "Epoch : 1 [59000/60000 (98%)]\tLoss: 0.556714\t Accuracy:89.448%\n",
            "Epoch : 1 [59500/60000 (99%)]\tLoss: 0.410622\t Accuracy:89.447%\n",
            "Epoch : 2 [0/60000 (0%)]\tLoss: 0.404183\t Accuracy:80.000%\n",
            "Epoch : 2 [500/60000 (1%)]\tLoss: 1.269007\t Accuracy:91.176%\n",
            "Epoch : 2 [1000/60000 (2%)]\tLoss: 0.439921\t Accuracy:90.495%\n",
            "Epoch : 2 [1500/60000 (2%)]\tLoss: 0.032439\t Accuracy:90.331%\n",
            "Epoch : 2 [2000/60000 (3%)]\tLoss: 0.080989\t Accuracy:90.249%\n",
            "Epoch : 2 [2500/60000 (4%)]\tLoss: 0.451153\t Accuracy:90.199%\n",
            "Epoch : 2 [3000/60000 (5%)]\tLoss: 0.378720\t Accuracy:89.834%\n",
            "Epoch : 2 [3500/60000 (6%)]\tLoss: 0.399272\t Accuracy:89.573%\n",
            "Epoch : 2 [4000/60000 (7%)]\tLoss: 0.096158\t Accuracy:89.601%\n",
            "Epoch : 2 [4500/60000 (8%)]\tLoss: 0.760641\t Accuracy:89.579%\n",
            "Epoch : 2 [5000/60000 (8%)]\tLoss: 0.167726\t Accuracy:89.780%\n",
            "Epoch : 2 [5500/60000 (9%)]\tLoss: 0.434896\t Accuracy:89.474%\n",
            "Epoch : 2 [6000/60000 (10%)]\tLoss: 0.484873\t Accuracy:89.501%\n",
            "Epoch : 2 [6500/60000 (11%)]\tLoss: 0.387522\t Accuracy:89.508%\n",
            "Epoch : 2 [7000/60000 (12%)]\tLoss: 0.481451\t Accuracy:89.544%\n",
            "Epoch : 2 [7500/60000 (12%)]\tLoss: 0.323556\t Accuracy:89.507%\n",
            "Epoch : 2 [8000/60000 (13%)]\tLoss: 0.357422\t Accuracy:89.526%\n",
            "Epoch : 2 [8500/60000 (14%)]\tLoss: 0.142871\t Accuracy:89.612%\n",
            "Epoch : 2 [9000/60000 (15%)]\tLoss: 0.199761\t Accuracy:89.612%\n",
            "Epoch : 2 [9500/60000 (16%)]\tLoss: 0.306450\t Accuracy:89.621%\n",
            "Epoch : 2 [10000/60000 (17%)]\tLoss: 0.161340\t Accuracy:89.670%\n",
            "Epoch : 2 [10500/60000 (18%)]\tLoss: 0.008889\t Accuracy:89.772%\n",
            "Epoch : 2 [11000/60000 (18%)]\tLoss: 0.190981\t Accuracy:89.764%\n",
            "Epoch : 2 [11500/60000 (19%)]\tLoss: 0.302748\t Accuracy:89.896%\n",
            "Epoch : 2 [12000/60000 (20%)]\tLoss: 0.321515\t Accuracy:89.784%\n",
            "Epoch : 2 [12500/60000 (21%)]\tLoss: 0.220951\t Accuracy:89.784%\n",
            "Epoch : 2 [13000/60000 (22%)]\tLoss: 0.120638\t Accuracy:89.839%\n",
            "Epoch : 2 [13500/60000 (22%)]\tLoss: 0.079818\t Accuracy:89.845%\n",
            "Epoch : 2 [14000/60000 (23%)]\tLoss: 0.048288\t Accuracy:89.772%\n",
            "Epoch : 2 [14500/60000 (24%)]\tLoss: 0.177750\t Accuracy:89.779%\n",
            "Epoch : 2 [15000/60000 (25%)]\tLoss: 0.051723\t Accuracy:89.840%\n",
            "Epoch : 2 [15500/60000 (26%)]\tLoss: 0.194092\t Accuracy:89.813%\n",
            "Epoch : 2 [16000/60000 (27%)]\tLoss: 1.141786\t Accuracy:89.844%\n",
            "Epoch : 2 [16500/60000 (28%)]\tLoss: 0.241024\t Accuracy:89.849%\n",
            "Epoch : 2 [17000/60000 (28%)]\tLoss: 0.101092\t Accuracy:89.818%\n",
            "Epoch : 2 [17500/60000 (29%)]\tLoss: 0.171144\t Accuracy:89.823%\n",
            "Epoch : 2 [18000/60000 (30%)]\tLoss: 1.139731\t Accuracy:89.806%\n",
            "Epoch : 2 [18500/60000 (31%)]\tLoss: 0.085991\t Accuracy:89.827%\n",
            "Epoch : 2 [19000/60000 (32%)]\tLoss: 0.632224\t Accuracy:89.853%\n",
            "Epoch : 2 [19500/60000 (32%)]\tLoss: 0.065771\t Accuracy:89.862%\n",
            "Epoch : 2 [20000/60000 (33%)]\tLoss: 0.141565\t Accuracy:89.890%\n",
            "Epoch : 2 [20500/60000 (34%)]\tLoss: 0.057936\t Accuracy:89.883%\n",
            "Epoch : 2 [21000/60000 (35%)]\tLoss: 0.118390\t Accuracy:89.914%\n",
            "Epoch : 2 [21500/60000 (36%)]\tLoss: 0.352374\t Accuracy:89.893%\n",
            "Epoch : 2 [22000/60000 (37%)]\tLoss: 0.021435\t Accuracy:89.945%\n",
            "Epoch : 2 [22500/60000 (38%)]\tLoss: 0.494631\t Accuracy:89.956%\n",
            "Epoch : 2 [23000/60000 (38%)]\tLoss: 0.077692\t Accuracy:89.957%\n",
            "Epoch : 2 [23500/60000 (39%)]\tLoss: 0.130215\t Accuracy:89.953%\n",
            "Epoch : 2 [24000/60000 (40%)]\tLoss: 0.457724\t Accuracy:90.017%\n",
            "Epoch : 2 [24500/60000 (41%)]\tLoss: 0.254338\t Accuracy:90.029%\n",
            "Epoch : 2 [25000/60000 (42%)]\tLoss: 0.174566\t Accuracy:90.032%\n",
            "Epoch : 2 [25500/60000 (42%)]\tLoss: 0.358002\t Accuracy:90.074%\n",
            "Epoch : 2 [26000/60000 (43%)]\tLoss: 0.127770\t Accuracy:90.046%\n",
            "Epoch : 2 [26500/60000 (44%)]\tLoss: 0.090598\t Accuracy:90.030%\n",
            "Epoch : 2 [27000/60000 (45%)]\tLoss: 0.007930\t Accuracy:90.063%\n",
            "Epoch : 2 [27500/60000 (46%)]\tLoss: 0.265888\t Accuracy:90.073%\n",
            "Epoch : 2 [28000/60000 (47%)]\tLoss: 0.557950\t Accuracy:90.089%\n",
            "Epoch : 2 [28500/60000 (48%)]\tLoss: 0.348534\t Accuracy:90.098%\n",
            "Epoch : 2 [29000/60000 (48%)]\tLoss: 0.038340\t Accuracy:90.110%\n",
            "Epoch : 2 [29500/60000 (49%)]\tLoss: 0.214678\t Accuracy:90.122%\n",
            "Epoch : 2 [30000/60000 (50%)]\tLoss: 0.009371\t Accuracy:90.083%\n",
            "Epoch : 2 [30500/60000 (51%)]\tLoss: 0.305622\t Accuracy:90.108%\n",
            "Epoch : 2 [31000/60000 (52%)]\tLoss: 0.060432\t Accuracy:90.077%\n",
            "Epoch : 2 [31500/60000 (52%)]\tLoss: 0.187487\t Accuracy:90.048%\n",
            "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.544770\t Accuracy:90.069%\n",
            "Epoch : 2 [32500/60000 (54%)]\tLoss: 0.199524\t Accuracy:90.058%\n",
            "Epoch : 2 [33000/60000 (55%)]\tLoss: 0.085625\t Accuracy:90.064%\n",
            "Epoch : 2 [33500/60000 (56%)]\tLoss: 0.898136\t Accuracy:90.084%\n",
            "Epoch : 2 [34000/60000 (57%)]\tLoss: 0.462506\t Accuracy:90.056%\n",
            "Epoch : 2 [34500/60000 (58%)]\tLoss: 0.150479\t Accuracy:90.058%\n",
            "Epoch : 2 [35000/60000 (58%)]\tLoss: 0.335951\t Accuracy:90.066%\n",
            "Epoch : 2 [35500/60000 (59%)]\tLoss: 0.672394\t Accuracy:90.045%\n",
            "Epoch : 2 [36000/60000 (60%)]\tLoss: 0.031779\t Accuracy:90.061%\n",
            "Epoch : 2 [36500/60000 (61%)]\tLoss: 0.266931\t Accuracy:90.036%\n",
            "Epoch : 2 [37000/60000 (62%)]\tLoss: 0.496066\t Accuracy:90.027%\n",
            "Epoch : 2 [37500/60000 (62%)]\tLoss: 0.293296\t Accuracy:90.019%\n",
            "Epoch : 2 [38000/60000 (63%)]\tLoss: 0.373676\t Accuracy:90.047%\n",
            "Epoch : 2 [38500/60000 (64%)]\tLoss: 0.162747\t Accuracy:90.070%\n",
            "Epoch : 2 [39000/60000 (65%)]\tLoss: 0.075940\t Accuracy:90.085%\n",
            "Epoch : 2 [39500/60000 (66%)]\tLoss: 0.096125\t Accuracy:90.081%\n",
            "Epoch : 2 [40000/60000 (67%)]\tLoss: 0.215887\t Accuracy:90.077%\n",
            "Epoch : 2 [40500/60000 (68%)]\tLoss: 0.211721\t Accuracy:90.069%\n",
            "Epoch : 2 [41000/60000 (68%)]\tLoss: 0.107350\t Accuracy:90.044%\n",
            "Epoch : 2 [41500/60000 (69%)]\tLoss: 0.295903\t Accuracy:90.029%\n",
            "Epoch : 2 [42000/60000 (70%)]\tLoss: 0.084720\t Accuracy:90.021%\n",
            "Epoch : 2 [42500/60000 (71%)]\tLoss: 0.252021\t Accuracy:90.005%\n",
            "Epoch : 2 [43000/60000 (72%)]\tLoss: 0.261109\t Accuracy:90.026%\n",
            "Epoch : 2 [43500/60000 (72%)]\tLoss: 0.397669\t Accuracy:90.032%\n",
            "Epoch : 2 [44000/60000 (73%)]\tLoss: 0.141569\t Accuracy:90.052%\n",
            "Epoch : 2 [44500/60000 (74%)]\tLoss: 0.377739\t Accuracy:90.049%\n",
            "Epoch : 2 [45000/60000 (75%)]\tLoss: 0.441636\t Accuracy:90.029%\n",
            "Epoch : 2 [45500/60000 (76%)]\tLoss: 0.192755\t Accuracy:90.007%\n",
            "Epoch : 2 [46000/60000 (77%)]\tLoss: 0.385468\t Accuracy:90.017%\n",
            "Epoch : 2 [46500/60000 (78%)]\tLoss: 0.212946\t Accuracy:90.009%\n",
            "Epoch : 2 [47000/60000 (78%)]\tLoss: 0.040049\t Accuracy:90.026%\n",
            "Epoch : 2 [47500/60000 (79%)]\tLoss: 1.066560\t Accuracy:90.013%\n",
            "Epoch : 2 [48000/60000 (80%)]\tLoss: 0.105106\t Accuracy:90.023%\n",
            "Epoch : 2 [48500/60000 (81%)]\tLoss: 0.048267\t Accuracy:90.033%\n",
            "Epoch : 2 [49000/60000 (82%)]\tLoss: 0.422033\t Accuracy:90.022%\n",
            "Epoch : 2 [49500/60000 (82%)]\tLoss: 0.735925\t Accuracy:90.016%\n",
            "Epoch : 2 [50000/60000 (83%)]\tLoss: 0.018112\t Accuracy:90.014%\n",
            "Epoch : 2 [50500/60000 (84%)]\tLoss: 0.103625\t Accuracy:90.012%\n",
            "Epoch : 2 [51000/60000 (85%)]\tLoss: 0.468666\t Accuracy:90.012%\n",
            "Epoch : 2 [51500/60000 (86%)]\tLoss: 0.067191\t Accuracy:90.029%\n",
            "Epoch : 2 [52000/60000 (87%)]\tLoss: 0.114474\t Accuracy:90.015%\n",
            "Epoch : 2 [52500/60000 (88%)]\tLoss: 0.612883\t Accuracy:90.015%\n",
            "Epoch : 2 [53000/60000 (88%)]\tLoss: 0.566369\t Accuracy:90.002%\n",
            "Epoch : 2 [53500/60000 (89%)]\tLoss: 0.435295\t Accuracy:90.002%\n",
            "Epoch : 2 [54000/60000 (90%)]\tLoss: 0.068102\t Accuracy:89.987%\n",
            "Epoch : 2 [54500/60000 (91%)]\tLoss: 0.307895\t Accuracy:89.965%\n",
            "Epoch : 2 [55000/60000 (92%)]\tLoss: 0.265150\t Accuracy:89.969%\n",
            "Epoch : 2 [55500/60000 (92%)]\tLoss: 0.220473\t Accuracy:89.977%\n",
            "Epoch : 2 [56000/60000 (93%)]\tLoss: 0.035210\t Accuracy:89.970%\n",
            "Epoch : 2 [56500/60000 (94%)]\tLoss: 0.477756\t Accuracy:89.963%\n",
            "Epoch : 2 [57000/60000 (95%)]\tLoss: 0.176029\t Accuracy:89.953%\n",
            "Epoch : 2 [57500/60000 (96%)]\tLoss: 0.800960\t Accuracy:89.957%\n",
            "Epoch : 2 [58000/60000 (97%)]\tLoss: 0.210607\t Accuracy:89.959%\n",
            "Epoch : 2 [58500/60000 (98%)]\tLoss: 0.088454\t Accuracy:89.956%\n",
            "Epoch : 2 [59000/60000 (98%)]\tLoss: 0.282685\t Accuracy:89.951%\n",
            "Epoch : 2 [59500/60000 (99%)]\tLoss: 0.300400\t Accuracy:89.958%\n",
            "Epoch : 3 [0/60000 (0%)]\tLoss: 0.287230\t Accuracy:80.000%\n",
            "Epoch : 3 [500/60000 (1%)]\tLoss: 1.122958\t Accuracy:91.176%\n",
            "Epoch : 3 [1000/60000 (2%)]\tLoss: 0.365129\t Accuracy:90.792%\n",
            "Epoch : 3 [1500/60000 (2%)]\tLoss: 0.036578\t Accuracy:90.265%\n",
            "Epoch : 3 [2000/60000 (3%)]\tLoss: 0.060962\t Accuracy:90.448%\n",
            "Epoch : 3 [2500/60000 (4%)]\tLoss: 0.497788\t Accuracy:90.159%\n",
            "Epoch : 3 [3000/60000 (5%)]\tLoss: 0.407368\t Accuracy:89.867%\n",
            "Epoch : 3 [3500/60000 (6%)]\tLoss: 0.420062\t Accuracy:89.744%\n",
            "Epoch : 3 [4000/60000 (7%)]\tLoss: 0.072904\t Accuracy:89.651%\n",
            "Epoch : 3 [4500/60000 (8%)]\tLoss: 0.655238\t Accuracy:89.756%\n",
            "Epoch : 3 [5000/60000 (8%)]\tLoss: 0.218260\t Accuracy:90.000%\n",
            "Epoch : 3 [5500/60000 (9%)]\tLoss: 0.190675\t Accuracy:89.855%\n",
            "Epoch : 3 [6000/60000 (10%)]\tLoss: 0.441402\t Accuracy:90.033%\n",
            "Epoch : 3 [6500/60000 (11%)]\tLoss: 0.272450\t Accuracy:90.015%\n",
            "Epoch : 3 [7000/60000 (12%)]\tLoss: 0.367228\t Accuracy:90.128%\n",
            "Epoch : 3 [7500/60000 (12%)]\tLoss: 0.337569\t Accuracy:90.133%\n",
            "Epoch : 3 [8000/60000 (13%)]\tLoss: 0.369994\t Accuracy:90.175%\n",
            "Epoch : 3 [8500/60000 (14%)]\tLoss: 0.143001\t Accuracy:90.212%\n",
            "Epoch : 3 [9000/60000 (15%)]\tLoss: 0.103525\t Accuracy:90.122%\n",
            "Epoch : 3 [9500/60000 (16%)]\tLoss: 0.226314\t Accuracy:90.189%\n",
            "Epoch : 3 [10000/60000 (17%)]\tLoss: 0.169007\t Accuracy:90.210%\n",
            "Epoch : 3 [10500/60000 (18%)]\tLoss: 0.013110\t Accuracy:90.304%\n",
            "Epoch : 3 [11000/60000 (18%)]\tLoss: 0.186783\t Accuracy:90.263%\n",
            "Epoch : 3 [11500/60000 (19%)]\tLoss: 0.224952\t Accuracy:90.365%\n",
            "Epoch : 3 [12000/60000 (20%)]\tLoss: 0.303663\t Accuracy:90.325%\n",
            "Epoch : 3 [12500/60000 (21%)]\tLoss: 0.188667\t Accuracy:90.328%\n",
            "Epoch : 3 [13000/60000 (22%)]\tLoss: 0.082465\t Accuracy:90.323%\n",
            "Epoch : 3 [13500/60000 (22%)]\tLoss: 0.042654\t Accuracy:90.303%\n",
            "Epoch : 3 [14000/60000 (23%)]\tLoss: 0.040561\t Accuracy:90.243%\n",
            "Epoch : 3 [14500/60000 (24%)]\tLoss: 0.143725\t Accuracy:90.269%\n",
            "Epoch : 3 [15000/60000 (25%)]\tLoss: 0.047283\t Accuracy:90.333%\n",
            "Epoch : 3 [15500/60000 (26%)]\tLoss: 0.218950\t Accuracy:90.348%\n",
            "Epoch : 3 [16000/60000 (27%)]\tLoss: 1.199072\t Accuracy:90.375%\n",
            "Epoch : 3 [16500/60000 (28%)]\tLoss: 0.245187\t Accuracy:90.357%\n",
            "Epoch : 3 [17000/60000 (28%)]\tLoss: 0.120962\t Accuracy:90.341%\n",
            "Epoch : 3 [17500/60000 (29%)]\tLoss: 0.106139\t Accuracy:90.326%\n",
            "Epoch : 3 [18000/60000 (30%)]\tLoss: 0.866682\t Accuracy:90.294%\n",
            "Epoch : 3 [18500/60000 (31%)]\tLoss: 0.123326\t Accuracy:90.292%\n",
            "Epoch : 3 [19000/60000 (32%)]\tLoss: 0.466330\t Accuracy:90.295%\n",
            "Epoch : 3 [19500/60000 (32%)]\tLoss: 0.067829\t Accuracy:90.323%\n",
            "Epoch : 3 [20000/60000 (33%)]\tLoss: 0.160804\t Accuracy:90.360%\n",
            "Epoch : 3 [20500/60000 (34%)]\tLoss: 0.087070\t Accuracy:90.332%\n",
            "Epoch : 3 [21000/60000 (35%)]\tLoss: 0.248329\t Accuracy:90.366%\n",
            "Epoch : 3 [21500/60000 (36%)]\tLoss: 0.508933\t Accuracy:90.307%\n",
            "Epoch : 3 [22000/60000 (37%)]\tLoss: 0.017132\t Accuracy:90.377%\n",
            "Epoch : 3 [22500/60000 (38%)]\tLoss: 0.306829\t Accuracy:90.395%\n",
            "Epoch : 3 [23000/60000 (38%)]\tLoss: 0.056469\t Accuracy:90.391%\n",
            "Epoch : 3 [23500/60000 (39%)]\tLoss: 0.151220\t Accuracy:90.387%\n",
            "Epoch : 3 [24000/60000 (40%)]\tLoss: 0.584081\t Accuracy:90.412%\n",
            "Epoch : 3 [24500/60000 (41%)]\tLoss: 0.270193\t Accuracy:90.428%\n",
            "Epoch : 3 [25000/60000 (42%)]\tLoss: 0.330666\t Accuracy:90.420%\n",
            "Epoch : 3 [25500/60000 (42%)]\tLoss: 0.443607\t Accuracy:90.451%\n",
            "Epoch : 3 [26000/60000 (43%)]\tLoss: 0.120173\t Accuracy:90.458%\n",
            "Epoch : 3 [26500/60000 (44%)]\tLoss: 0.083039\t Accuracy:90.460%\n",
            "Epoch : 3 [27000/60000 (45%)]\tLoss: 0.013617\t Accuracy:90.500%\n",
            "Epoch : 3 [27500/60000 (46%)]\tLoss: 0.438658\t Accuracy:90.534%\n",
            "Epoch : 3 [28000/60000 (47%)]\tLoss: 0.726710\t Accuracy:90.532%\n",
            "Epoch : 3 [28500/60000 (48%)]\tLoss: 0.301471\t Accuracy:90.540%\n",
            "Epoch : 3 [29000/60000 (48%)]\tLoss: 0.036334\t Accuracy:90.534%\n",
            "Epoch : 3 [29500/60000 (49%)]\tLoss: 0.318995\t Accuracy:90.542%\n",
            "Epoch : 3 [30000/60000 (50%)]\tLoss: 0.004765\t Accuracy:90.533%\n",
            "Epoch : 3 [30500/60000 (51%)]\tLoss: 0.427034\t Accuracy:90.547%\n",
            "Epoch : 3 [31000/60000 (52%)]\tLoss: 0.066410\t Accuracy:90.519%\n",
            "Epoch : 3 [31500/60000 (52%)]\tLoss: 0.195103\t Accuracy:90.473%\n",
            "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.447172\t Accuracy:90.503%\n",
            "Epoch : 3 [32500/60000 (54%)]\tLoss: 0.270493\t Accuracy:90.492%\n",
            "Epoch : 3 [33000/60000 (55%)]\tLoss: 0.080470\t Accuracy:90.506%\n",
            "Epoch : 3 [33500/60000 (56%)]\tLoss: 0.962325\t Accuracy:90.522%\n",
            "Epoch : 3 [34000/60000 (57%)]\tLoss: 0.447404\t Accuracy:90.503%\n",
            "Epoch : 3 [34500/60000 (58%)]\tLoss: 0.111131\t Accuracy:90.519%\n",
            "Epoch : 3 [35000/60000 (58%)]\tLoss: 0.404132\t Accuracy:90.534%\n",
            "Epoch : 3 [35500/60000 (59%)]\tLoss: 0.645091\t Accuracy:90.524%\n",
            "Epoch : 3 [36000/60000 (60%)]\tLoss: 0.060719\t Accuracy:90.525%\n",
            "Epoch : 3 [36500/60000 (61%)]\tLoss: 0.326275\t Accuracy:90.507%\n",
            "Epoch : 3 [37000/60000 (62%)]\tLoss: 0.577746\t Accuracy:90.511%\n",
            "Epoch : 3 [37500/60000 (62%)]\tLoss: 0.263445\t Accuracy:90.504%\n",
            "Epoch : 3 [38000/60000 (63%)]\tLoss: 0.299460\t Accuracy:90.518%\n",
            "Epoch : 3 [38500/60000 (64%)]\tLoss: 0.151574\t Accuracy:90.548%\n",
            "Epoch : 3 [39000/60000 (65%)]\tLoss: 0.110979\t Accuracy:90.561%\n",
            "Epoch : 3 [39500/60000 (66%)]\tLoss: 0.121287\t Accuracy:90.562%\n",
            "Epoch : 3 [40000/60000 (67%)]\tLoss: 0.234881\t Accuracy:90.567%\n",
            "Epoch : 3 [40500/60000 (68%)]\tLoss: 0.218603\t Accuracy:90.546%\n",
            "Epoch : 3 [41000/60000 (68%)]\tLoss: 0.133186\t Accuracy:90.549%\n",
            "Epoch : 3 [41500/60000 (69%)]\tLoss: 0.296282\t Accuracy:90.532%\n",
            "Epoch : 3 [42000/60000 (70%)]\tLoss: 0.072291\t Accuracy:90.533%\n",
            "Epoch : 3 [42500/60000 (71%)]\tLoss: 0.170742\t Accuracy:90.525%\n",
            "Epoch : 3 [43000/60000 (72%)]\tLoss: 0.281634\t Accuracy:90.535%\n",
            "Epoch : 3 [43500/60000 (72%)]\tLoss: 0.406080\t Accuracy:90.529%\n",
            "Epoch : 3 [44000/60000 (73%)]\tLoss: 0.150508\t Accuracy:90.548%\n",
            "Epoch : 3 [44500/60000 (74%)]\tLoss: 0.360527\t Accuracy:90.555%\n",
            "Epoch : 3 [45000/60000 (75%)]\tLoss: 0.353018\t Accuracy:90.558%\n",
            "Epoch : 3 [45500/60000 (76%)]\tLoss: 0.113274\t Accuracy:90.543%\n",
            "Epoch : 3 [46000/60000 (77%)]\tLoss: 0.268171\t Accuracy:90.546%\n",
            "Epoch : 3 [46500/60000 (78%)]\tLoss: 0.274514\t Accuracy:90.520%\n",
            "Epoch : 3 [47000/60000 (78%)]\tLoss: 0.061565\t Accuracy:90.523%\n",
            "Epoch : 3 [47500/60000 (79%)]\tLoss: 0.892843\t Accuracy:90.507%\n",
            "Epoch : 3 [48000/60000 (80%)]\tLoss: 0.130129\t Accuracy:90.517%\n",
            "Epoch : 3 [48500/60000 (81%)]\tLoss: 0.052464\t Accuracy:90.517%\n",
            "Epoch : 3 [49000/60000 (82%)]\tLoss: 0.383104\t Accuracy:90.516%\n",
            "Epoch : 3 [49500/60000 (82%)]\tLoss: 0.567134\t Accuracy:90.507%\n",
            "Epoch : 3 [50000/60000 (83%)]\tLoss: 0.006199\t Accuracy:90.510%\n",
            "Epoch : 3 [50500/60000 (84%)]\tLoss: 0.107690\t Accuracy:90.523%\n",
            "Epoch : 3 [51000/60000 (85%)]\tLoss: 0.422215\t Accuracy:90.525%\n",
            "Epoch : 3 [51500/60000 (86%)]\tLoss: 0.119238\t Accuracy:90.532%\n",
            "Epoch : 3 [52000/60000 (87%)]\tLoss: 0.111539\t Accuracy:90.515%\n",
            "Epoch : 3 [52500/60000 (88%)]\tLoss: 0.548355\t Accuracy:90.522%\n",
            "Epoch : 3 [53000/60000 (88%)]\tLoss: 0.623998\t Accuracy:90.523%\n",
            "Epoch : 3 [53500/60000 (89%)]\tLoss: 0.361273\t Accuracy:90.521%\n",
            "Epoch : 3 [54000/60000 (90%)]\tLoss: 0.042978\t Accuracy:90.505%\n",
            "Epoch : 3 [54500/60000 (91%)]\tLoss: 0.191718\t Accuracy:90.490%\n",
            "Epoch : 3 [55000/60000 (92%)]\tLoss: 0.247645\t Accuracy:90.484%\n",
            "Epoch : 3 [55500/60000 (92%)]\tLoss: 0.203221\t Accuracy:90.477%\n",
            "Epoch : 3 [56000/60000 (93%)]\tLoss: 0.037924\t Accuracy:90.477%\n",
            "Epoch : 3 [56500/60000 (94%)]\tLoss: 0.497977\t Accuracy:90.471%\n",
            "Epoch : 3 [57000/60000 (95%)]\tLoss: 0.147808\t Accuracy:90.472%\n",
            "Epoch : 3 [57500/60000 (96%)]\tLoss: 0.619706\t Accuracy:90.480%\n",
            "Epoch : 3 [58000/60000 (97%)]\tLoss: 0.212694\t Accuracy:90.488%\n",
            "Epoch : 3 [58500/60000 (98%)]\tLoss: 0.109614\t Accuracy:90.470%\n",
            "Epoch : 3 [59000/60000 (98%)]\tLoss: 0.341150\t Accuracy:90.464%\n",
            "Epoch : 3 [59500/60000 (99%)]\tLoss: 0.237664\t Accuracy:90.467%\n",
            "Epoch : 4 [0/60000 (0%)]\tLoss: 0.243348\t Accuracy:90.000%\n",
            "Epoch : 4 [500/60000 (1%)]\tLoss: 1.092133\t Accuracy:91.961%\n",
            "Epoch : 4 [1000/60000 (2%)]\tLoss: 0.498773\t Accuracy:90.990%\n",
            "Epoch : 4 [1500/60000 (2%)]\tLoss: 0.018990\t Accuracy:90.464%\n",
            "Epoch : 4 [2000/60000 (3%)]\tLoss: 0.058161\t Accuracy:90.697%\n",
            "Epoch : 4 [2500/60000 (4%)]\tLoss: 0.297320\t Accuracy:90.558%\n",
            "Epoch : 4 [3000/60000 (5%)]\tLoss: 0.442675\t Accuracy:90.199%\n",
            "Epoch : 4 [3500/60000 (6%)]\tLoss: 0.311036\t Accuracy:89.943%\n",
            "Epoch : 4 [4000/60000 (7%)]\tLoss: 0.077570\t Accuracy:90.075%\n",
            "Epoch : 4 [4500/60000 (8%)]\tLoss: 0.806126\t Accuracy:90.200%\n",
            "Epoch : 4 [5000/60000 (8%)]\tLoss: 0.327192\t Accuracy:90.459%\n",
            "Epoch : 4 [5500/60000 (9%)]\tLoss: 0.196942\t Accuracy:90.218%\n",
            "Epoch : 4 [6000/60000 (10%)]\tLoss: 0.359853\t Accuracy:90.466%\n",
            "Epoch : 4 [6500/60000 (11%)]\tLoss: 0.077450\t Accuracy:90.522%\n",
            "Epoch : 4 [7000/60000 (12%)]\tLoss: 0.399104\t Accuracy:90.528%\n",
            "Epoch : 4 [7500/60000 (12%)]\tLoss: 0.337779\t Accuracy:90.533%\n",
            "Epoch : 4 [8000/60000 (13%)]\tLoss: 0.432413\t Accuracy:90.574%\n",
            "Epoch : 4 [8500/60000 (14%)]\tLoss: 0.123994\t Accuracy:90.552%\n",
            "Epoch : 4 [9000/60000 (15%)]\tLoss: 0.122150\t Accuracy:90.466%\n",
            "Epoch : 4 [9500/60000 (16%)]\tLoss: 0.207987\t Accuracy:90.494%\n",
            "Epoch : 4 [10000/60000 (17%)]\tLoss: 0.193813\t Accuracy:90.500%\n",
            "Epoch : 4 [10500/60000 (18%)]\tLoss: 0.004544\t Accuracy:90.618%\n",
            "Epoch : 4 [11000/60000 (18%)]\tLoss: 0.139755\t Accuracy:90.590%\n",
            "Epoch : 4 [11500/60000 (19%)]\tLoss: 0.235665\t Accuracy:90.678%\n",
            "Epoch : 4 [12000/60000 (20%)]\tLoss: 0.237613\t Accuracy:90.600%\n",
            "Epoch : 4 [12500/60000 (21%)]\tLoss: 0.207846\t Accuracy:90.600%\n",
            "Epoch : 4 [13000/60000 (22%)]\tLoss: 0.143606\t Accuracy:90.615%\n",
            "Epoch : 4 [13500/60000 (22%)]\tLoss: 0.043037\t Accuracy:90.637%\n",
            "Epoch : 4 [14000/60000 (23%)]\tLoss: 0.032785\t Accuracy:90.578%\n",
            "Epoch : 4 [14500/60000 (24%)]\tLoss: 0.141716\t Accuracy:90.593%\n",
            "Epoch : 4 [15000/60000 (25%)]\tLoss: 0.060616\t Accuracy:90.646%\n",
            "Epoch : 4 [15500/60000 (26%)]\tLoss: 0.142648\t Accuracy:90.632%\n",
            "Epoch : 4 [16000/60000 (27%)]\tLoss: 1.415348\t Accuracy:90.650%\n",
            "Epoch : 4 [16500/60000 (28%)]\tLoss: 0.241610\t Accuracy:90.636%\n",
            "Epoch : 4 [17000/60000 (28%)]\tLoss: 0.181943\t Accuracy:90.623%\n",
            "Epoch : 4 [17500/60000 (29%)]\tLoss: 0.158314\t Accuracy:90.577%\n",
            "Epoch : 4 [18000/60000 (30%)]\tLoss: 0.570111\t Accuracy:90.589%\n",
            "Epoch : 4 [18500/60000 (31%)]\tLoss: 0.055679\t Accuracy:90.567%\n",
            "Epoch : 4 [19000/60000 (32%)]\tLoss: 0.322802\t Accuracy:90.558%\n",
            "Epoch : 4 [19500/60000 (32%)]\tLoss: 0.037140\t Accuracy:90.595%\n",
            "Epoch : 4 [20000/60000 (33%)]\tLoss: 0.145233\t Accuracy:90.620%\n",
            "Epoch : 4 [20500/60000 (34%)]\tLoss: 0.024957\t Accuracy:90.624%\n",
            "Epoch : 4 [21000/60000 (35%)]\tLoss: 0.008808\t Accuracy:90.638%\n",
            "Epoch : 4 [21500/60000 (36%)]\tLoss: 0.377274\t Accuracy:90.614%\n",
            "Epoch : 4 [22000/60000 (37%)]\tLoss: 0.009895\t Accuracy:90.650%\n",
            "Epoch : 4 [22500/60000 (38%)]\tLoss: 0.048498\t Accuracy:90.675%\n",
            "Epoch : 4 [23000/60000 (38%)]\tLoss: 0.114574\t Accuracy:90.639%\n",
            "Epoch : 4 [23500/60000 (39%)]\tLoss: 0.187417\t Accuracy:90.621%\n",
            "Epoch : 4 [24000/60000 (40%)]\tLoss: 0.363825\t Accuracy:90.679%\n",
            "Epoch : 4 [24500/60000 (41%)]\tLoss: 0.310505\t Accuracy:90.706%\n",
            "Epoch : 4 [25000/60000 (42%)]\tLoss: 0.253446\t Accuracy:90.700%\n",
            "Epoch : 4 [25500/60000 (42%)]\tLoss: 0.428896\t Accuracy:90.729%\n",
            "Epoch : 4 [26000/60000 (43%)]\tLoss: 0.144440\t Accuracy:90.727%\n",
            "Epoch : 4 [26500/60000 (44%)]\tLoss: 0.139343\t Accuracy:90.709%\n",
            "Epoch : 4 [27000/60000 (45%)]\tLoss: 0.004410\t Accuracy:90.752%\n",
            "Epoch : 4 [27500/60000 (46%)]\tLoss: 0.236303\t Accuracy:90.800%\n",
            "Epoch : 4 [28000/60000 (47%)]\tLoss: 0.618423\t Accuracy:90.800%\n",
            "Epoch : 4 [28500/60000 (48%)]\tLoss: 0.299152\t Accuracy:90.824%\n",
            "Epoch : 4 [29000/60000 (48%)]\tLoss: 0.040532\t Accuracy:90.824%\n",
            "Epoch : 4 [29500/60000 (49%)]\tLoss: 0.238435\t Accuracy:90.834%\n",
            "Epoch : 4 [30000/60000 (50%)]\tLoss: 0.006255\t Accuracy:90.806%\n",
            "Epoch : 4 [30500/60000 (51%)]\tLoss: 0.349070\t Accuracy:90.829%\n",
            "Epoch : 4 [31000/60000 (52%)]\tLoss: 0.035641\t Accuracy:90.800%\n",
            "Epoch : 4 [31500/60000 (52%)]\tLoss: 0.187372\t Accuracy:90.752%\n",
            "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.428658\t Accuracy:90.781%\n",
            "Epoch : 4 [32500/60000 (54%)]\tLoss: 0.159021\t Accuracy:90.778%\n",
            "Epoch : 4 [33000/60000 (55%)]\tLoss: 0.068201\t Accuracy:90.806%\n",
            "Epoch : 4 [33500/60000 (56%)]\tLoss: 0.823169\t Accuracy:90.812%\n",
            "Epoch : 4 [34000/60000 (57%)]\tLoss: 0.555233\t Accuracy:90.779%\n",
            "Epoch : 4 [34500/60000 (58%)]\tLoss: 0.062866\t Accuracy:90.791%\n",
            "Epoch : 4 [35000/60000 (58%)]\tLoss: 0.345560\t Accuracy:90.788%\n",
            "Epoch : 4 [35500/60000 (59%)]\tLoss: 0.469672\t Accuracy:90.783%\n",
            "Epoch : 4 [36000/60000 (60%)]\tLoss: 0.055413\t Accuracy:90.797%\n",
            "Epoch : 4 [36500/60000 (61%)]\tLoss: 0.284376\t Accuracy:90.775%\n",
            "Epoch : 4 [37000/60000 (62%)]\tLoss: 0.451305\t Accuracy:90.775%\n",
            "Epoch : 4 [37500/60000 (62%)]\tLoss: 0.266941\t Accuracy:90.765%\n",
            "Epoch : 4 [38000/60000 (63%)]\tLoss: 0.350288\t Accuracy:90.776%\n",
            "Epoch : 4 [38500/60000 (64%)]\tLoss: 0.170192\t Accuracy:90.821%\n",
            "Epoch : 4 [39000/60000 (65%)]\tLoss: 0.064932\t Accuracy:90.838%\n",
            "Epoch : 4 [39500/60000 (66%)]\tLoss: 0.101164\t Accuracy:90.838%\n",
            "Epoch : 4 [40000/60000 (67%)]\tLoss: 0.247251\t Accuracy:90.825%\n",
            "Epoch : 4 [40500/60000 (68%)]\tLoss: 0.289570\t Accuracy:90.812%\n",
            "Epoch : 4 [41000/60000 (68%)]\tLoss: 0.050553\t Accuracy:90.824%\n",
            "Epoch : 4 [41500/60000 (69%)]\tLoss: 0.226861\t Accuracy:90.819%\n",
            "Epoch : 4 [42000/60000 (70%)]\tLoss: 0.085749\t Accuracy:90.812%\n",
            "Epoch : 4 [42500/60000 (71%)]\tLoss: 0.286500\t Accuracy:90.786%\n",
            "Epoch : 4 [43000/60000 (72%)]\tLoss: 0.316695\t Accuracy:90.795%\n",
            "Epoch : 4 [43500/60000 (72%)]\tLoss: 0.364952\t Accuracy:90.802%\n",
            "Epoch : 4 [44000/60000 (73%)]\tLoss: 0.145856\t Accuracy:90.827%\n",
            "Epoch : 4 [44500/60000 (74%)]\tLoss: 0.287871\t Accuracy:90.831%\n",
            "Epoch : 4 [45000/60000 (75%)]\tLoss: 0.270124\t Accuracy:90.840%\n",
            "Epoch : 4 [45500/60000 (76%)]\tLoss: 0.195024\t Accuracy:90.815%\n",
            "Epoch : 4 [46000/60000 (77%)]\tLoss: 0.289455\t Accuracy:90.815%\n",
            "Epoch : 4 [46500/60000 (78%)]\tLoss: 0.193598\t Accuracy:90.791%\n",
            "Epoch : 4 [47000/60000 (78%)]\tLoss: 0.035862\t Accuracy:90.789%\n",
            "Epoch : 4 [47500/60000 (79%)]\tLoss: 0.914925\t Accuracy:90.764%\n",
            "Epoch : 4 [48000/60000 (80%)]\tLoss: 0.130396\t Accuracy:90.787%\n",
            "Epoch : 4 [48500/60000 (81%)]\tLoss: 0.081587\t Accuracy:90.806%\n",
            "Epoch : 4 [49000/60000 (82%)]\tLoss: 0.428605\t Accuracy:90.808%\n",
            "Epoch : 4 [49500/60000 (82%)]\tLoss: 0.641403\t Accuracy:90.796%\n",
            "Epoch : 4 [50000/60000 (83%)]\tLoss: 0.009345\t Accuracy:90.794%\n",
            "Epoch : 4 [50500/60000 (84%)]\tLoss: 0.065319\t Accuracy:90.792%\n",
            "Epoch : 4 [51000/60000 (85%)]\tLoss: 0.445705\t Accuracy:90.794%\n",
            "Epoch : 4 [51500/60000 (86%)]\tLoss: 0.100938\t Accuracy:90.788%\n",
            "Epoch : 4 [52000/60000 (87%)]\tLoss: 0.124324\t Accuracy:90.775%\n",
            "Epoch : 4 [52500/60000 (88%)]\tLoss: 0.276699\t Accuracy:90.779%\n",
            "Epoch : 4 [53000/60000 (88%)]\tLoss: 0.501497\t Accuracy:90.770%\n",
            "Epoch : 4 [53500/60000 (89%)]\tLoss: 0.283902\t Accuracy:90.774%\n",
            "Epoch : 4 [54000/60000 (90%)]\tLoss: 0.029064\t Accuracy:90.765%\n",
            "Epoch : 4 [54500/60000 (91%)]\tLoss: 0.144175\t Accuracy:90.748%\n",
            "Epoch : 4 [55000/60000 (92%)]\tLoss: 0.241503\t Accuracy:90.731%\n",
            "Epoch : 4 [55500/60000 (92%)]\tLoss: 0.160814\t Accuracy:90.726%\n",
            "Epoch : 4 [56000/60000 (93%)]\tLoss: 0.067011\t Accuracy:90.728%\n",
            "Epoch : 4 [56500/60000 (94%)]\tLoss: 0.600694\t Accuracy:90.720%\n",
            "Epoch : 4 [57000/60000 (95%)]\tLoss: 0.160687\t Accuracy:90.724%\n",
            "Epoch : 4 [57500/60000 (96%)]\tLoss: 0.592085\t Accuracy:90.737%\n",
            "Epoch : 4 [58000/60000 (97%)]\tLoss: 0.370224\t Accuracy:90.736%\n",
            "Epoch : 4 [58500/60000 (98%)]\tLoss: 0.106147\t Accuracy:90.720%\n",
            "Epoch : 4 [59000/60000 (98%)]\tLoss: 0.177678\t Accuracy:90.705%\n",
            "Epoch : 4 [59500/60000 (99%)]\tLoss: 0.275944\t Accuracy:90.718%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TBY4Y7FYYdr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "a5be1ae0-8253-462d-c354-0ef2a6188f53"
      },
      "cell_type": "code",
      "source": [
        "#Let's test our model on test set\n",
        "\n",
        "test_set = vision.datasets.FashionMNIST(\n",
        "              root='./data/FashionMNIST',\n",
        "              train = False ,\n",
        "              download = True,\n",
        "              transform = transforms.Compose([transforms.ToTensor()])\n",
        "                )\n",
        "\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:01, 14828670.02it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 103386.53it/s]           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 4370563.89it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 33167.79it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Xf_eVTyeQZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1477
        },
        "outputId": "837b9eb3-2a2b-4708-c8cd-1f78e0487f2b"
      },
      "cell_type": "code",
      "source": [
        "#We can see our test_set data now\n",
        "\n",
        "test_set[0]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0039, 0.0000, 0.0000, 0.0275, 0.0000, 0.1451,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0039, 0.0078, 0.0000, 0.1059, 0.3294, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4667,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0039, 0.0000, 0.0000, 0.3451, 0.5608, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.3647, 0.4157,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0157, 0.0000, 0.2078, 0.5059, 0.4706, 0.5765, 0.6863, 0.6157, 0.6510, 0.5294, 0.6039, 0.6588, 0.5490,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078,\n",
              "           0.0000, 0.0431, 0.5373, 0.5098, 0.5020, 0.6275, 0.6902, 0.6235, 0.6549, 0.6980, 0.5843, 0.5922, 0.5647,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0078, 0.0039, 0.0000, 0.0118, 0.0000,\n",
              "           0.0000, 0.4510, 0.4471, 0.4157, 0.5373, 0.6588, 0.6000, 0.6118, 0.6471, 0.6549, 0.5608, 0.6157, 0.6196,\n",
              "           0.0431, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.0000,\n",
              "           0.3490, 0.5451, 0.3529, 0.3686, 0.6000, 0.5843, 0.5137, 0.5922, 0.6627, 0.6745, 0.5608, 0.6235, 0.6627,\n",
              "           0.1882, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0157, 0.0039, 0.0000, 0.0000, 0.0000, 0.3843,\n",
              "           0.5333, 0.4314, 0.4275, 0.4314, 0.6353, 0.5294, 0.5647, 0.5843, 0.6235, 0.6549, 0.5647, 0.6196, 0.6627,\n",
              "           0.4667, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0078, 0.0078, 0.0039, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.1020, 0.4235, 0.4588,\n",
              "           0.3882, 0.4353, 0.4588, 0.5333, 0.6118, 0.5255, 0.6039, 0.6039, 0.6118, 0.6275, 0.5529, 0.5765, 0.6118,\n",
              "           0.6980, 0.0000],\n",
              "          [0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824, 0.2078, 0.3608, 0.4588, 0.4353, 0.4039,\n",
              "           0.4510, 0.5059, 0.5255, 0.5608, 0.6039, 0.6471, 0.6667, 0.6039, 0.5922, 0.6039, 0.5608, 0.5412, 0.5882,\n",
              "           0.6471, 0.1686],\n",
              "          [0.0000, 0.0000, 0.0902, 0.2118, 0.2549, 0.2980, 0.3333, 0.4627, 0.5020, 0.4824, 0.4353, 0.4431, 0.4627,\n",
              "           0.4980, 0.4902, 0.5451, 0.5216, 0.5333, 0.6275, 0.5490, 0.6078, 0.6314, 0.5647, 0.6078, 0.6745, 0.6314,\n",
              "           0.7412, 0.2431],\n",
              "          [0.0000, 0.2667, 0.3686, 0.3529, 0.4353, 0.4471, 0.4353, 0.4471, 0.4510, 0.4980, 0.5294, 0.5333, 0.5608,\n",
              "           0.4941, 0.4980, 0.5922, 0.6039, 0.5608, 0.5804, 0.4902, 0.6353, 0.6353, 0.5647, 0.5412, 0.6000, 0.6353,\n",
              "           0.7686, 0.2275],\n",
              "          [0.2745, 0.6627, 0.5059, 0.4078, 0.3843, 0.3922, 0.3686, 0.3804, 0.3843, 0.4000, 0.4235, 0.4157, 0.4667,\n",
              "           0.4706, 0.5059, 0.5843, 0.6118, 0.6549, 0.7451, 0.7451, 0.7686, 0.7765, 0.7765, 0.7333, 0.7725, 0.7412,\n",
              "           0.7216, 0.1412],\n",
              "          [0.0627, 0.4941, 0.6706, 0.7373, 0.7373, 0.7216, 0.6706, 0.6000, 0.5294, 0.4706, 0.4941, 0.4980, 0.5725,\n",
              "           0.7255, 0.7647, 0.8196, 0.8157, 1.0000, 0.8196, 0.6941, 0.9608, 0.9882, 0.9843, 0.9843, 0.9686, 0.8627,\n",
              "           0.8078, 0.1922],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0471, 0.2627, 0.4157, 0.6431, 0.7255, 0.7804, 0.8235, 0.8275, 0.8235, 0.8157,\n",
              "           0.7451, 0.5882, 0.3216, 0.0314, 0.0000, 0.0000, 0.0000, 0.6980, 0.8157, 0.7373, 0.6863, 0.6353, 0.6196,\n",
              "           0.5922, 0.0431],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000]]]), 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "metadata": {
        "id": "h2rNalV2eav2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loading the Test Set Data to DataLoader\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set , batch_size=10)\n",
        "#Image will be of tensor size [10,1,28,28]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PG2ICJQ0e1hp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45df6c99-2116-46d7-8c71-cf7792e33f09"
      },
      "cell_type": "code",
      "source": [
        "def evaluate_test(model):\n",
        "#model = mlp\n",
        "    correct = 0 \n",
        "    for test_imgs, test_labels in test_loader:\n",
        "        #print(test_imgs.shape)\n",
        "        test_imgs = Variable(test_imgs).float()\n",
        "        output = model(test_imgs)\n",
        "        predicted = torch.max(output,1)[1]\n",
        "        correct += (predicted == test_labels).sum()\n",
        "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_loader)*BATCH_SIZE)))\n",
        "evaluate_test(mlp)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:0.882% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mFAqzZIUfH5D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}